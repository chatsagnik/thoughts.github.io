<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An Introduction to Reductions | Theoretickles</title><meta name=keywords content="reductions,p-np,SAT,halting problem,undecidability"><meta name=description content="
This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let&rsquo;s dive in.

Introduction
The What and Why of reductions?
From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one &lsquo;Eureka!&rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.
Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:"><meta name=author content="Me"><link rel=canonical href=https://theoretickles.netlify.app/posts/reductions/><link crossorigin=anonymous href=/assets/css/stylesheet.bfcd547679eb0c9e77b9520047f99b465519837cdf010548f39350d378f73b68.css integrity="sha256-v81UdnnrDJ53uVIAR/mbRlUZg3zfAQVI85NQ03j3O2g=" rel="preload stylesheet" as=style><link rel=icon href=https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://theoretickles.netlify.app/posts/reductions/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/javascript id=MathJax-script async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://theoretickles.netlify.app/posts/reductions/"><meta property="og:site_name" content="Theoretickles"><meta property="og:title" content="An Introduction to Reductions"><meta property="og:description" content=" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let‚Äôs dive in.
Introduction The What and Why of reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one ‚ÄòEureka!‚Äô at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs. Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-01T11:51:18+05:30"><meta property="article:modified_time" content="2025-01-01T11:51:18+05:30"><meta property="article:tag" content="Reductions"><meta property="article:tag" content="P-Np"><meta property="article:tag" content="SAT"><meta property="article:tag" content="Halting Problem"><meta property="article:tag" content="Undecidability"><meta property="og:image" content="https://theoretickles.netlify.app/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://theoretickles.netlify.app/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="An Introduction to Reductions"><meta name=twitter:description content="
This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let&rsquo;s dive in.

Introduction
The What and Why of reductions?
From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one &lsquo;Eureka!&rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.
Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://theoretickles.netlify.app/posts/"},{"@type":"ListItem","position":2,"name":"An Introduction to Reductions","item":"https://theoretickles.netlify.app/posts/reductions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An Introduction to Reductions","name":"An Introduction to Reductions","description":" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let\u0026rsquo;s dive in.\nIntroduction The What and Why of reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs. Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\n","keywords":["reductions","p-np","SAT","halting problem","undecidability"],"articleBody":" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let‚Äôs dive in.\nIntroduction The What and Why of reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one ‚ÄòEureka!‚Äô at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs. Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\nMost unsolved problems we encounter are already related to some existing problem we know about.\nThis relation to solved problems is something that we can now prod and prick incessantly and eventually use to arrive at a conclusion about new problems. While hardly elegant in its conception, this approach is highly sophisticated in its execution. This is the basis for the mathematical technique known as reduction.\nWhen can we use reduction techniques? First we start by describing two types of problems: hard and easy! Hard problems are typically those that the problem solver does not know how to solve or knows are very hard to solve, while easy problems are simply easy to solve.\nSince the notion of hardness as described above is quite subjective and non-rigorous, we formalize it below by quantifying the capabilities of the problem solver.\nThroughout this post, we assume that the notion of hard and easy depends only on the computational resources available to the problem solver.\nAccess to powerful computational models might allow us (the problem solver) to efficiently find solutions to previously hard-to-solve problems. For example, SAT can be easily solved if we have know how to solve the Halting problem. We shall expand on this example later in this post.\nTake two problems1, $O$ (an old problem) and $N$ (a new problem). Suppose we make the astute observation that the underlying mathematical structure of $N$ is similar to $O$. Therefore, instead of trying to solve $N$ from scratch, we can hope to somehow use $O$ to solve $N$. Consider the following situations of interest that might arise where we would ideally like to avoid reinventing the wheel.\nCase 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve In this case, mathematically, we say that $N\\subseteq O$. In this case, our primary approach is to transform instances of $N$ into instances of $O$ and then use the algorithm for $O$ to solve for $N$. In other words, if we can demonstrate that $N$ is a special case of $O$, we solve $N$ using our existing knowledge of $O$.\nReducing problem $A$ to $B$ is the process of transforming instances of $A$ into instances of $B$. Note that this is only possible if $A\\subseteq B$.\nCase 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve In this case, we can again exploit the structural similarities between $O$ and $N$, but this time to a different end. Without prior context, it is difficult to show that $N$ is hard to solve. However, using the hardness of $O$, it is easy to show that $N$ is hard to solve as well.\nWe start with the assumption that $N$ is easy to solve, i.e., there exists some algorithm to efficiently decide membership in the set $N$. We use this assumption to arrive at a contradiction. Now exploiting the similarities between $O$ and $N$, we transform all instances of $O$ to some instance of $N$.\nSince $O$ reduces to $N$, i.e., there is a way to efficiently decide membership in $O$ using the algorithm for $N$. However, we know for sure that $O$ is hard to solve. This leads us to a contradiction, which is resolved by removing our assumption that $N$ is easy to solve. Therefore $N$ has to be hard to solve as well.\nLet us explore a few properties of reductions now.\nRelative Hardness For any pair of languages / decision problems $A$ and $B$, if $A\\subseteq B$, then $B$ is at least as hard as $A$ to solve, i.e., $A$ cannot be harder to solve than $B$.\nReductions as relations Reflexivity of reductions: Trivially $A\\subseteq A$ by using identity transformations. Transitivity of reductions: If $A\\subseteq B$ and $B\\subseteq C$ then we can prove that $A\\subseteq C$ by composing the transformations. Reductions are, however, not symmetric relations (and, therefore, by extension, not equivalence relations). $A\\subseteq B$ does not imply $B\\subseteq A$. A concrete example of this will be discussed later in detail: We can reduce SAT, which is a decidable problem, to the Halting problem, which is an undecidable problem, but the converse is not true (by definition of decidability).\nAn alternate view of reductions Reductions are a highly general family of techniques, and to provide a rigorous formalization of reductions, we consider some specific variants. An interesting way of looking at reductions is as follows:\nGiven an oracle for a problem $B$, can we solve $A$ efficiently by making oracle calls to $B$? If yes, then $A\\subseteq B$.\nHere, an oracle is nothing but a black-box subroutine for efficiently solving a problem. The beauty of reductions lie in the fact that we do not need to bother about the internal mechanisms of the oracle itself. Nor do we have to worry about construct the oracle itself. We simply have to use the oracle‚Äôs existence. Implicitly, reduction is a two-step process. Suppose $A\\subseteq B$.\nSuppose there exists a subroutine $R$ that transforms yes instances of $A$ into yes instances of $B$, and no instances of $A$ into no instances of $B$. First, input instances $x$ (that may/may not belong to $A$) are reduced/transformed into possible instances of $B$ using the subroutine $R$. Next, we perform invocation(s) to the oracle for $B$ to decide the membership of $R(x)$ in $B$. Explicitly, we perform the following computation: $O_B(R(x))$. If $O_B(R(x))=1$, then we decide that $x\\in A$. Otherwise, we decide that $x\\notin A$. $$x\\in A \\iff R(x) \\in B.$$ The notion of efficiency is twofold.\nFirstly, we are concerned with how efficient the transformation/reduction from $x$ to $R(x)$ is. The notion of efficiency in transforming $x$ to $R(x)$ has a small caveat:\nOnly the construction of the reduction method $R$ needs to be efficient, and solving $O_B(R(x))$ does not need to be efficient. Sometimes, we might require more than one call to the oracle $O_B$, depending on the problems at hand. In this case, we are concerned with how many times the oracle is invoked.\nWe explore the notion of efficiency in reductions via an example.\nExample of an ‚Äúefficient‚Äù reduction Consider the following C pseudocode:\nint A(int x){ for(;x\u003e12;x++); return x; // This is R(x) } Writing this piece of code took finite time, and was certainly efficient. However, depending on the value of the input the for loop will never terminate for values of $x\u003e11$. Therefore, this program may become inefficient during runtime. The notion of efficiency we consider during reductions (unlike computational scenarios) is how efficiently we can write the code for function A, and not how efficiently A transforms $x$ into $R(x)$. A Prototypical Reduction: SAT to the Halting Problem Note: This is a highly technical section, and would need familiarity to the satisfiability problem and the Halting problem. We encourage the reader to familiarize themselves with the formal definitions of these problems first before going through this section. We give an intuitive description of the problems below.\nThe Halting Problem Recall the earlier piece of pseudocode with a slight modification.\nint A1(int x){ for(;x\u003e12;x++); return x; // This is R(x) } int A2(int x){ for(;x\u003e12;x--);// This loop will always terminate return x; // This is R(x) } Note A2 will always halt no matter the input, while A1 may never halt depending on the input.\nImagine you would like to design a function $B$ that takes the binary description of any single parameter function $A$ and any arbitrary input $x$, and find out whether $A$ will halt on input $x$.\nThe Halting problem (HALT): Given (the binary encoding of) any arbitrary Turing Machine $A$ and an arbitrary input (encoded as a binary string) $x$, does $A(x)$ halt?\nThis in effect describes the Halting problem, where $B$ is a universal Turing machine and $A$ can be any Turing Machine. It has been shown that there does not exist any such $B$ which solves this problem. Therefore the Halting problem cannot be decided, and is formally referred to as an undecidable problem.\nThe SAT Problem A Boolean formula $f$ accepts as input an $n$-bit string and outputs a $1$ (if it accepts the string), or a $0$ (if it rejects the string). Mathematically, $f:{0,1}^{n}\\xrightarrow{}{0,1}$.\nThe satisfiability(SAT) problem: Given a Boolean formula $f$ and an arbitrary $n$ bit string $x\\in{0,1}^{n}$, is $f(x)=1$?\nNote that the structure of SAT is subsumed by the structure of the Halting problem. If the decider to Halting problem ever halts, we can find out whether $f(x)=1$.\nEven though SAT can be quite hard to solve computationally (may have exponential runtime depending on the structure of the formula), we can always construct an algorithm to solve it. Therefore, right off the bat, we observe that SAT is easier to solve (it is decidable) than the Halting problem.\nReducing SAT to HALT Let us finally have a look at how we would reduce SAT to the Halting problem.\nOur input $x$ is a Boolean formula. We want to output if this formula is satisfiable.\nWe construct a TM (Turing Machine) $T$ which accepts $x$ and does the following:\n$T$ iterates over all possible assignments to find a satisfying assignment. üî¥ This may require exponential runtime in the size of the formula.\nIf $T$ finds a satisfying assignment, halt and return 1. üü£ Hence, if x is satisfiable, T halts. Otherwise, we put $T$ into an infinite loop. üü£ Hence, T halts iff if x is satisfiable.\nOur reduction $R(x)=\\langle\\langle T\\rangle,x\\rangle$ takes the SAT formula $x$ and returns an encoding of the above Turing machine $T$ coupled with $x$, s.t., yes instances of SAT map to yes instances of HALT, and no instances of SAT map to no instances of HALT. üü£ Note that $R(x)$ at this point can be compared to a compiled binary which has not yet been executed.\nWe pass $R(x)$ to $O_{\\text{HALT}}$.\nIf $O_{\\text{HALT}}(R(x))$ returns yes, this implies $T$ halts on input $x$, which in turn implies $x$ has a satisfying assignment. Therefore $R(x)\\in\\text{HALT}\\implies x\\in\\text{SAT}$. If $O_{\\text{HALT}}(R(x))$ returns no, then $T$ does not halt on input $x$, which implies that $x$ does not have a satisfying assignment. Therefore $R(x)\\notin\\text{HALT}\\implies x\\notin\\text{SAT}$. We can take the contrapositive of this expression to obtain $x\\in\\text{SAT}\\implies R(x)\\in\\text{HALT}$. Once again we note the following (recall this).\nIn step 2, we are simply constructing the TM $T$, not executing it. Think of this as writing a C or Java program/executable for $T$. However, we are never actually going into runtime; i.e. executing the executable at any point.\nThe reduction is the transformation of the SAT formula $x$ to an encoding of both the Turing Machine and the SAT formula, which is an instance of the Halting problem (HALT$_{TM}$ requires an arbitrary TM and an input string to the TM).\nTaxonomy of Polynomial-time reductions In this post, we only consider polynomial-time deterministic reductions, i.e.,\nthe time taken to transform $x$ to $R(x)$ using a DTM, and the number of calls to $O_B$ to decide membership of $R(x)$ in $B$, are both polynomial. These are the most commonly studied types of reductions,2 and we look at three kinds of polynomial-time reductions.\nKarp Reductions / Many-one reductions These are the most restrictive type of polynomial reductions. Given a single input $x$, $R(x)$ produces a single instance $y$ such that $x\\in A\\iff y\\in B$. Therefore, we have to perform only one oracle call to $O_B$. The earlier reduction from SAT to HALT was a Karp reduction.\nWe can choose to strengthen the notion of Karp reductions to include weaker forms of reductions. For example, in logspace many-one reductions, we can compute $R(x)$ using just logarithmic space instead of polynomial time. Even more restrictive notions of reductions consider reductions computable by constant depth circuit classes.\nTruth Table Reductions These are reductions in which given a single input $x$, $R(x)$ produces a constant number of outputs $y_1,y_2,\\ldots, y_k$ to $B$. The output $O_A(x)$ can be expressed in terms of a function $f$ that combines the outputs $O_B(y_i)$ for $i\\in[1,\\ldots,k]$.\nLet us assume that $f$ outputs $1$ for the desired combination and $0$ otherwise. $$ x\\in A\\iff f\\left(y_1\\in B, y_1\\in B,\\ldots,y_k\\in B\\right)=1 $$ Here, $f$ is efficiently computable, and there are a constant ($k$) number of oracle calls to $O_B$.\nLet us consider an example. Consider two problems on a graph $G$ with a constant number of vertices $\\ell$.\n$A$: What is the minimum sized independent set for $G$? $B$: Does $G$ have an independent set of size $k$?\nThe reduction $A\\subseteq B$ would involve looping from $1$ to $\\ell$ and querying $B$ each time. The combination function would be an OR function. At the first yes instance of $B$, we return the value as the answer for $A$. If there is no independent set in $G$, the worst number of calls to $O_B$ is $\\ell$.\nCook Reductions / Poly-time Turing Reductions Here, we are allowed a polynomial number of oracle calls and polynomial time for transforming the inputs. These are the most general form of reductions, and the other forms of reductions are restrictions of Cook reductions. In the example graph $G$ used for TT reductions, if we assume the number of vertices of $G$ to be a polynomial, then the reduction $A\\subseteq B$ using the same exact process would be a Cook reduction.\n$A\\subseteq_{m} B \\implies A\\subseteq_{t} B \\implies A\\subseteq_{T} B$ where, $\\subseteq_{m}$ denotes Karp reductions, $\\subseteq_{t}$ denotes Truth Table reductions, and $\\subseteq_{T}$ denotes Cook reductions.\nAside: From the nature of the examples, we can see that Karp reductions only extend to decision problems (problems with yes/no outputs). In contrast, Cook reductions can accommodate search/relation/optimization problems (problems with a set of outputs).\nBasic reductions in Computability Theory In this section, the reader is assumed to have familiarity with concepts of decidability and undecidability. Let us now proceed with some instances of reductions in computability theory.\nLet $A\\subseteq B$, and they are decision problems.\nIf $A$ is decidable, what can we say about $B$? If $A$ is semi-decidable, what can we say about $B$? If $A$ is undecidable, what can we say about $B$? We know that $B$ is at least as hard as $A$ since $A\\subseteq B$. Therefore in the first case, $B$ may be decidable, semi-decidable, or undecidable. In the second case, $B$ may be semi-decidable, or undecidable. In the third case, $B$ is undecidable.\nThe third case is of interest here - To show that a $B$ is undecidable, we must find a reduction from $A$ to $B$, where $A$ is already known to be undecidable. Note (Advanced): The reduction function must itself be a computable function.\nNow again, consider $A\\subseteq B$ where they are decision problems. We can conclude the following:\nIf $B$ is decidable, $A$ is decidable. If $B$ is semi-decidable, $A$ is semi-decidable. If $A$ is not decidable, then $B$ is not decidable. If $A$ is not semi-decidable, then $B$ is not semi-decidable. $A\\subseteq B$, then $\\bar{A}\\subseteq \\bar{B}$, where $A$ and $B$ are decidable problems. The first two statements are true, as discussed above, while 3 is the contrapositive of the first statement and the fourth statement is the contrapositive of the second statement. For the fifth statement, the proof is as follows:\n$x\\in \\bar{A} \\iff x\\notin A \\iff R(x)\\notin B \\iff R(x)\\in\\bar{B}$. Therefore, $x\\in \\bar{A} \\iff R(x)\\in\\bar{B}$.\nImmediately we see the power of formalizing the notion of reductions.\nComplexity-Theoretic notions This section is for advanced readers only due to the number of prerequisites involved.\nA complexity class is a set of computational problems that can be solved using similar amounts of bounded resources (time, space, circuit depth, number of gates, etc.) on a given computational model (Turing machines, circuits, cellular automaton, etc.).\nThe complexity classes $\\mathrm{P}$, $\\mathrm{NP}$, and $\\mathrm{PSPACE}$ are closed under Karp and logspace reductions. The complexity classes $\\mathrm{L}$ and $\\mathrm{NL}$ are closed only under logspace reductions. Closure has the following semantics - given a decision problem $A$ in a complexity class $C$, any problem $B$ such that $B\\subseteq A$ is also in $C$.\nWe now explore two interesting notions in complexity theory that arise from reductions.\nCompleteness For a bounded complexity class,\nComplete problems are the hardest problems inside their respective complexity class.\nA more formal definition of completeness is as follows:\nGiven a complexity class $C$ which is closed under reduction $r$, if there exists a problem $A$ in $C$ such that all other problems in $C$ are $r$-reducible to $A$, $A$ is said to be C-complete.\nFor example, an NP-complete problem is in NP, and all problems in NP are Karp-reducible to it. The notions of PSPACE-completeness and EXPTIME-completeness are similarly defined under Karp-reductions.\nThe role of reductions in this context can be understood through P-completeness. Consider any non-trivial decision problem in P (trivial problems are akin to constant functions). Every other non-trivial decision problem is Karp-reducible to it. Therefore every non-trivial decision problem is P-complete under Karp-reductions. This definition is essentially the same as P (minus the empty language and $\\Sigma^*$).\nTherefore, we come to the following conclusion:\nUnder Karp-reductions, the notion of P-completeness is semantically useless.\nHence, we use weaker forms of reductions such as logspace reductions or reductions using constant depth circuit computable functions to achieve a more meaningful notion of P-completeness 3.\nSelf reducibility Decision problems are yes/no problems where we ask if a solution exists. However, sometimes we also want to solve the corresponding search problem to find a solution (if one exists).\nIn the context of languages in NP, self-reducibility essentially states that if we can efficiently solve the decision version of a problem, we can also efficiently solve the search/optimization version of the problem. A more formal definition would be as follows:\nThe search version of a problem Cook-reduces (polynomial-time Turing-reduces) to the decision version of the problem.\nFact: Every NP-complete problem is self-reducible and downward self-reducible4.\nConclusion This write-up aims to demystify a core technique used in theoretical computer science and provide a few contexts for its usage. For a more rigorous and formal introduction to this topic, please check out Michael Sipser‚Äôs excellent book ‚ÄúAn Introduction to the Theory of Computation.‚Äù There are many more things to say about reductions, and this article barely scratches the surface on any of the topics it touches. However, I feel it would be prudent to stop at this point before it becomes any more of a rambling mess than it already is.\nFormally, we consider problems $O$ and $N$ to be languages or sets of strings corresponding to decision problems, i.e., YES/NO problems.¬†‚Ü©Ô∏é\nWe will look at the notion of randomized reductions in another post.¬†‚Ü©Ô∏é\nWe will look at weaker notions of reductions in another post.¬†‚Ü©Ô∏é\nWe will look at the notions of self-reducibility and downward self-reducibility in another post.¬†‚Ü©Ô∏é\n","wordCount":"3230","inLanguage":"en","image":"https://theoretickles.netlify.app/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-01-01T11:51:18+05:30","dateModified":"2025-01-01T11:51:18+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://theoretickles.netlify.app/posts/reductions/"},"publisher":{"@type":"Organization","name":"Theoretickles","logo":{"@type":"ImageObject","url":"https://theoretickles.netlify.app/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://theoretickles.netlify.app/ accesskey=h title="Theoretickles (Alt + H)"><img src=https://theoretickles.netlify.app/apple-touch-icon.png alt aria-label=logo height=35>Theoretickles</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://theoretickles.netlify.app/categories/ title=categories><span>categories</span></a></li><li><a href=https://theoretickles.netlify.app/tags/ title=tags><span>tags</span></a></li><li><a href=https://chatsagnik.github.io title=chatsagnik><span>chatsagnik</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://theoretickles.netlify.app/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://theoretickles.netlify.app/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://theoretickles.netlify.app/>Home</a>&nbsp;¬ª&nbsp;<a href=https://theoretickles.netlify.app/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">An Introduction to Reductions</h1><div class=post-meta><span title='2025-01-01 11:51:18 +0530 IST'>January 1, 2025</span>&nbsp;¬∑&nbsp;16 min&nbsp;¬∑&nbsp;3230 words&nbsp;¬∑&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/reductions.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#the-what-and-why-of-reductions aria-label="The What and Why of reductions?">The What and Why of reductions?</a></li><li><a href=#when-can-we-use-reduction-techniques aria-label="When can we use reduction techniques?">When can we use reduction techniques?</a><ul><li><a href=#case-1-o-is-harder-to-solve-than-n-but-o-is-actually-easy-to-solve aria-label="Case 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve">Case 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve</a></li><li><a href=#case-2-o-is-easier-to-solve-than-n-but-o-is-actually-hard-to-solve aria-label="Case 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve">Case 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve</a></li><li><a href=#relative-hardness aria-label="Relative Hardness">Relative Hardness</a></li><li><a href=#reductions-as-relations aria-label="Reductions as relations">Reductions as relations</a></li></ul></li></ul></li><li><a href=#an-alternate-view-of-reductions aria-label="An alternate view of reductions">An alternate view of reductions</a><ul><li><a href=#example-of-an-efficient-reduction aria-label="Example of an &ldquo;efficient&rdquo; reduction">Example of an &ldquo;efficient&rdquo; reduction</a></li></ul></li><li><a href=#a-prototypical-reduction-sat-to-the-halting-problem aria-label="A Prototypical Reduction: SAT to the Halting Problem">A Prototypical Reduction: SAT to the Halting Problem</a><ul><li><a href=#the-halting-problem aria-label="The Halting Problem">The Halting Problem</a></li><li><a href=#the-sat-problem aria-label="The SAT Problem">The SAT Problem</a></li><li><a href=#reducing-sat-to-halt aria-label="Reducing SAT to HALT">Reducing SAT to HALT</a></li></ul></li><li><a href=#taxonomy-of-polynomial-time-reductions aria-label="Taxonomy of Polynomial-time reductions">Taxonomy of Polynomial-time reductions</a><ul><li><a href=#karp-reductions--many-one-reductions aria-label="Karp Reductions / Many-one reductions">Karp Reductions / Many-one reductions</a></li><li><a href=#truth-table-reductions aria-label="Truth Table Reductions">Truth Table Reductions</a></li><li><a href=#cook-reductions--poly-time-turing-reductions aria-label="Cook Reductions / Poly-time Turing Reductions">Cook Reductions / Poly-time Turing Reductions</a></li></ul></li><li><a href=#basic-reductions-in-computability-theory aria-label="Basic reductions in Computability Theory">Basic reductions in Computability Theory</a></li><li><a href=#complexity-theoretic-notions aria-label="Complexity-Theoretic notions">Complexity-Theoretic notions</a></li><li><a href=#completeness aria-label=Completeness>Completeness</a></li><li><a href=#self-reducibility aria-label="Self reducibility">Self reducibility</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><blockquote><p><em>This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let&rsquo;s dive in.</em></p></blockquote><hr><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><h3 id=the-what-and-why-of-reductions>The What and Why of reductions?<a hidden class=anchor aria-hidden=true href=#the-what-and-why-of-reductions>#</a></h3><p>From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one &lsquo;Eureka!&rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.
Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:</p><blockquote><p>Most unsolved problems we encounter are already related to some existing problem we know about.</p></blockquote><p>This relation to solved problems is something that we can now prod and prick incessantly and eventually use to arrive at a conclusion about new problems. While hardly elegant in its conception, this approach is highly sophisticated in its execution. This is the basis for the mathematical technique known as reduction.</p><h3 id=when-can-we-use-reduction-techniques>When can we use reduction techniques?<a hidden class=anchor aria-hidden=true href=#when-can-we-use-reduction-techniques>#</a></h3><p>First we start by describing two types of problems: hard and easy! <strong>Hard problems</strong> are typically those that the problem solver <em>does not know how to solve</em> or <em>knows are very hard to solve</em>, while easy problems are simply <em>easy</em> to solve.</p><p>Since the notion of hardness as described above is quite subjective and non-rigorous, we formalize it below by quantifying the capabilities of the problem solver.</p><blockquote><p>Throughout this post, we assume that the notion of <em>hard</em> and <em>easy</em> depends only on the computational resources available to the problem solver.</p></blockquote><p>Access to powerful computational models might allow us (the problem solver) to efficiently find solutions to previously <strong>hard-to-solve</strong> problems. For example, <code>SAT</code> can be easily solved if we have know how to solve the <code>Halting problem</code>. We shall expand on this example <a href=#reducing-sat-to-the-halting-problem>later in this post</a>.</p><p>Take two problems<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, $O$ (<em>an old problem</em>) and $N$ (<em>a new problem</em>). Suppose we make the astute observation that the underlying mathematical structure of $N$ is similar to $O$. Therefore, instead of trying to solve $N$ from scratch, we can hope to somehow use $O$ to solve $N$. Consider the following situations of interest that might arise where we would ideally like to avoid reinventing the wheel.</p><h4 id=case-1-o-is-harder-to-solve-than-n-but-o-is-actually-easy-to-solve>Case 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve<a hidden class=anchor aria-hidden=true href=#case-1-o-is-harder-to-solve-than-n-but-o-is-actually-easy-to-solve>#</a></h4><p>In this case, mathematically, we say that $N\subseteq O$. In this case, our primary approach is to transform instances of $N$ into instances of $O$ and then use the algorithm for $O$ to solve for $N$. In other words, if we can demonstrate that $N$ is a special case of $O$, we solve $N$ using our existing knowledge of $O$.</p><blockquote><p>Reducing problem $A$ to $B$ is the process of transforming instances of $A$ into instances of $B$. Note that this is only possible if $A\subseteq B$.</p></blockquote><h4 id=case-2-o-is-easier-to-solve-than-n-but-o-is-actually-hard-to-solve>Case 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve<a hidden class=anchor aria-hidden=true href=#case-2-o-is-easier-to-solve-than-n-but-o-is-actually-hard-to-solve>#</a></h4><p>In this case, we can again exploit the structural similarities between $O$ and $N$, but this time to a different end. Without prior context, it is difficult to show that $N$ is hard to solve. However, using the hardness of $O$, it is easy to show that $N$ is hard to solve as well.</p><p>We start with the assumption that $N$ is easy to solve, i.e., there exists some algorithm to efficiently decide membership in the set $N$. We use this assumption to arrive at a contradiction. Now exploiting the similarities between $O$ and $N$, we transform all instances of $O$ to some instance of $N$.</p><p>Since $O$ reduces to $N$, i.e., there is a way to efficiently decide membership in $O$ using the algorithm for $N$. However, we <strong>know for sure</strong> that $O$ is hard to solve. This leads us to a contradiction, which is resolved by removing our assumption that $N$ is easy to solve. Therefore $N$ has to be hard to solve as well.</p><p>Let us explore a few properties of reductions now.</p><h4 id=relative-hardness>Relative Hardness<a hidden class=anchor aria-hidden=true href=#relative-hardness>#</a></h4><p>For any pair of languages / decision problems $A$ and $B$, if $A\subseteq B$, then $B$ is at least as hard as $A$ to solve, i.e., $A$ <em>cannot be harder to solve than</em> $B$.</p><h4 id=reductions-as-relations>Reductions as relations<a hidden class=anchor aria-hidden=true href=#reductions-as-relations>#</a></h4><ul><li><strong>Reflexivity of reductions:</strong> Trivially $A\subseteq A$ by using identity transformations.</li><li><strong>Transitivity of reductions:</strong> If $A\subseteq B$ and $B\subseteq C$ then we can prove that $A\subseteq C$ by composing the transformations.</li><li>Reductions are, however, <strong>not symmetric relations</strong> (and, therefore, by extension, not equivalence relations). $A\subseteq B$ does not imply $B\subseteq A$.</li></ul><p>A concrete example of this will be discussed later in detail: We can reduce <code>SAT</code>, which is a decidable problem, to the <code>Halting problem</code>, which is an undecidable problem, but the converse is not true (by definition of decidability).</p><hr><h2 id=an-alternate-view-of-reductions>An alternate view of reductions<a hidden class=anchor aria-hidden=true href=#an-alternate-view-of-reductions>#</a></h2><p>Reductions are a highly general family of techniques, and to provide a rigorous formalization of reductions, we consider some specific variants. An interesting way of looking at reductions is as follows:</p><blockquote><p>Given an <strong>oracle</strong> for a problem $B$, can we solve $A$ efficiently by making oracle calls to $B$? If yes, then $A\subseteq B$.</p></blockquote><p>Here, an oracle is nothing but a <em>black-box subroutine</em> for efficiently solving a problem. The beauty of reductions lie in the fact that we do not need to bother about the internal mechanisms of the oracle itself. Nor do we have to worry about construct the oracle itself. <strong>We simply have to use the oracle&rsquo;s existence</strong>. Implicitly, reduction is a two-step process. Suppose $A\subseteq B$.</p><ul><li>Suppose there exists a subroutine $R$ that transforms <strong>yes instances</strong> of $A$ into <strong>yes instances</strong> of $B$, and <strong>no instances</strong> of $A$ into <strong>no instances</strong> of $B$. First, input instances $x$ (that may/may not belong to $A$) are reduced/transformed into <em>possible instances</em> of $B$ using the subroutine $R$.</li><li>Next, we perform invocation(s) to the oracle for $B$ to decide the membership of $R(x)$ in $B$. Explicitly, we perform the following computation: $O_B(R(x))$. If $O_B(R(x))=1$, then we decide that $x\in A$. Otherwise, we decide that $x\notin A$. $$x\in A \iff R(x) \in B.$$</li></ul><p><img alt=Reductions loading=lazy src=/posts/reductions.png></p><p>The notion of efficiency is twofold.</p><ul><li><p>Firstly, we are concerned with <strong>how efficient the transformation/reduction</strong> from $x$ to $R(x)$ is. The notion of efficiency in transforming $x$ to $R(x)$ has a small caveat:</p><ul><li>Only the construction of the reduction method $R$ needs to be efficient, and solving $O_B(R(x))$ does not need to be efficient.</li></ul></li><li><p>Sometimes, we might require more than one call to the oracle $O_B$, depending on the problems at hand. In this case, we are concerned with <strong>how many times the oracle is invoked</strong>.</p></li></ul><p>We explore the notion of efficiency in reductions via an example.</p><h3 id=example-of-an-efficient-reduction>Example of an &ldquo;efficient&rdquo; reduction<a hidden class=anchor aria-hidden=true href=#example-of-an-efficient-reduction>#</a></h3><ul><li><p>Consider the following C pseudocode:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kt>int</span> <span class=nf>A</span><span class=p>(</span><span class=kt>int</span> <span class=n>x</span><span class=p>){</span>
</span></span><span class=line><span class=cl> <span class=k>for</span><span class=p>(;</span><span class=n>x</span><span class=o>&gt;</span><span class=mi>12</span><span class=p>;</span><span class=n>x</span><span class=o>++</span><span class=p>);</span>
</span></span><span class=line><span class=cl> <span class=k>return</span> <span class=n>x</span><span class=p>;</span> <span class=c1>// This is R(x)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><ul><li>Writing this piece of code took finite time, and was certainly efficient. However, depending on the value of the input the <code>for</code> loop will never terminate for values of $x>11$. Therefore, this program may become <em>inefficient during runtime</em>. The notion of efficiency we consider during reductions (<strong>unlike computational scenarios</strong>) is how efficiently we can write the code for function <code>A</code>, and not how efficiently <code>A</code> transforms $x$ into $R(x)$.</li></ul></li></ul><hr><h2 id=a-prototypical-reduction-sat-to-the-halting-problem>A Prototypical Reduction: SAT to the Halting Problem<a hidden class=anchor aria-hidden=true href=#a-prototypical-reduction-sat-to-the-halting-problem>#</a></h2><blockquote><p><strong>Note:</strong> This is a highly technical section, and would need familiarity to the satisfiability problem and the Halting problem. We encourage the reader to familiarize themselves with the formal definitions of these problems first before going through this section. We give an intuitive description of the problems below.</p></blockquote><h3 id=the-halting-problem>The Halting Problem<a hidden class=anchor aria-hidden=true href=#the-halting-problem>#</a></h3><p>Recall the earlier piece of pseudocode with a slight modification.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kt>int</span> <span class=nf>A1</span><span class=p>(</span><span class=kt>int</span> <span class=n>x</span><span class=p>){</span>
</span></span><span class=line><span class=cl> <span class=k>for</span><span class=p>(;</span><span class=n>x</span><span class=o>&gt;</span><span class=mi>12</span><span class=p>;</span><span class=n>x</span><span class=o>++</span><span class=p>);</span>
</span></span><span class=line><span class=cl> <span class=k>return</span> <span class=n>x</span><span class=p>;</span> <span class=c1>// This is R(x)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>A2</span><span class=p>(</span><span class=kt>int</span> <span class=n>x</span><span class=p>){</span>
</span></span><span class=line><span class=cl> <span class=k>for</span><span class=p>(;</span><span class=n>x</span><span class=o>&gt;</span><span class=mi>12</span><span class=p>;</span><span class=n>x</span><span class=o>--</span><span class=p>);</span><span class=c1>// This loop will always terminate
</span></span></span><span class=line><span class=cl><span class=c1></span> <span class=k>return</span> <span class=n>x</span><span class=p>;</span> <span class=c1>// This is R(x)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>Note <code>A2</code> will always halt no matter the input, while <code>A1</code> may never halt depending on the input.</p><p>Imagine you would like to design a function $B$ that takes the binary description of any single parameter function $A$ and any arbitrary input $x$, and find out whether $A$ will halt on input $x$.</p><blockquote><p><strong>The Halting problem (HALT):</strong> Given (the binary encoding of) any arbitrary Turing Machine $A$ and an arbitrary input (encoded as a binary string) $x$, does $A(x)$ halt?</p></blockquote><p>This in effect describes the Halting problem, where $B$ is a universal Turing machine and $A$ can be any Turing Machine. It has been shown that there does not exist any such $B$ which solves this problem. Therefore the Halting problem cannot be decided, and is formally referred to as an <strong>undecidable</strong> problem.</p><h3 id=the-sat-problem>The SAT Problem<a hidden class=anchor aria-hidden=true href=#the-sat-problem>#</a></h3><p>A Boolean formula $f$ accepts as input an $n$-bit string and outputs a $1$ (if it accepts the string), or a $0$ (if it rejects the string). Mathematically, $f:{0,1}^{n}\xrightarrow{}{0,1}$.</p><blockquote><p><strong>The satisfiability(SAT) problem:</strong> Given a Boolean formula $f$ and an arbitrary $n$ bit string $x\in{0,1}^{n}$, is $f(x)=1$?</p></blockquote><p>Note that the structure of SAT is subsumed by the structure of the Halting problem. If the decider to <code>Halting problem</code> ever halts, we can find out whether $f(x)=1$.</p><p>Even though <code>SAT</code> can be quite hard to solve computationally (may have exponential runtime depending on the structure of the formula), we can <strong>always</strong> construct an algorithm to solve it. Therefore, right off the bat, we observe that <code>SAT</code> is easier to solve (it is decidable) than the <code>Halting problem</code>.</p><h3 id=reducing-sat-to-halt>Reducing SAT to HALT<a hidden class=anchor aria-hidden=true href=#reducing-sat-to-halt>#</a></h3><p>Let us finally have a look at how we would reduce SAT to the Halting problem.</p><ol><li><p>Our input $x$ is a Boolean formula. We want to output if this formula is satisfiable.</p></li><li><p>We construct a TM (Turing Machine) $T$ which accepts $x$ and does the following:</p><ul><li><p>$T$ iterates over all possible assignments to find a satisfying assignment.
<code>üî¥ This may require exponential runtime in the size of the formula.</code></p><ul><li>If $T$ finds a satisfying assignment, halt and return 1. <code>üü£ Hence, if x is satisfiable, T halts.</code></li></ul></li><li><p>Otherwise, we put $T$ into an infinite loop. <code>üü£ Hence, T halts iff if x is satisfiable.</code></p></li></ul></li><li><p>Our reduction $R(x)=\langle\langle T\rangle,x\rangle$ takes the SAT formula $x$ and returns an <strong>encoding</strong> of the above Turing machine $T$ coupled with $x$, s.t., yes instances of SAT map to yes instances of HALT, and no instances of SAT map to no instances of HALT. <code>üü£ Note that $R(x)$ at this point can be compared to a compiled binary which has not yet been executed.</code></p></li><li><p>We pass $R(x)$ to $O_{\text{HALT}}$.</p><ul><li>If $O_{\text{HALT}}(R(x))$ returns yes, this implies $T$ halts on input $x$, which in turn implies $x$ has a satisfying assignment. Therefore $R(x)\in\text{HALT}\implies x\in\text{SAT}$.</li><li>If $O_{\text{HALT}}(R(x))$ returns no, then $T$ does not halt on input $x$, which implies that $x$ does not have a satisfying assignment. Therefore $R(x)\notin\text{HALT}\implies x\notin\text{SAT}$. We can take the contrapositive of this expression to obtain $x\in\text{SAT}\implies R(x)\in\text{HALT}$.</li></ul></li></ol><p>Once again we note the following (<a href=#example-of-an-efficient-reduction>recall this</a>).</p><blockquote><p>In step 2, we are simply constructing the TM $T$, not executing it. Think of this as writing a C or Java program/executable for $T$. However, we are never actually going into runtime; i.e. executing the executable at any point.</p></blockquote><p>The reduction is the transformation of the SAT formula $x$ to an encoding of both the Turing Machine and the SAT formula, which is an instance of the Halting problem (HALT$_{TM}$ requires an arbitrary TM and an input string to the TM).</p><hr><h2 id=taxonomy-of-polynomial-time-reductions>Taxonomy of Polynomial-time reductions<a hidden class=anchor aria-hidden=true href=#taxonomy-of-polynomial-time-reductions>#</a></h2><p>In this post, we only consider polynomial-time <strong>deterministic</strong> reductions, i.e.,</p><ul><li>the time taken to transform $x$ to $R(x)$ using a DTM, and</li><li>the number of calls to $O_B$ to decide membership of $R(x)$ in $B$,</li></ul><p>are both polynomial. These are the <em>most commonly studied types of reductions</em>,<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> and we look at three kinds of polynomial-time reductions.</p><hr><h3 id=karp-reductions--many-one-reductions>Karp Reductions / Many-one reductions<a hidden class=anchor aria-hidden=true href=#karp-reductions--many-one-reductions>#</a></h3><p>These are the <strong>most restrictive type</strong> of polynomial reductions.
Given a single input $x$, $R(x)$ produces a single instance $y$ such that $x\in A\iff y\in B$. Therefore, we have to perform only one oracle call to $O_B$. The earlier reduction from <code>SAT</code> to <code>HALT</code> was a Karp reduction.</p><p>We can choose to strengthen the notion of Karp reductions to include weaker forms of reductions.
For example, in logspace many-one reductions, we can compute $R(x)$ using just logarithmic space instead of polynomial time. Even more restrictive notions of reductions consider reductions computable by constant depth circuit classes.</p><hr><h3 id=truth-table-reductions>Truth Table Reductions<a hidden class=anchor aria-hidden=true href=#truth-table-reductions>#</a></h3><p>These are reductions in which given a single input $x$, $R(x)$ produces a constant number of outputs $y_1,y_2,\ldots, y_k$ to $B$. The output $O_A(x)$ can be expressed in terms of a function $f$ that combines the outputs $O_B(y_i)$ for $i\in[1,\ldots,k]$.</p><blockquote><p>Let us assume that $f$ outputs $1$ for the desired combination and $0$ otherwise.
$$ x\in A\iff f\left(y_1\in B, y_1\in B,\ldots,y_k\in B\right)=1 $$
Here, $f$ is efficiently computable, and there are a constant ($k$) number of oracle calls to $O_B$.</p></blockquote><p>Let us consider an example. Consider two problems on a graph $G$ with a constant number of vertices $\ell$.</p><blockquote><p>$A$: What is the minimum sized independent set for $G$?
$B$: Does $G$ have an independent set of size $k$?</p></blockquote><p>The reduction $A\subseteq B$ would involve looping from $1$ to $\ell$ and querying $B$ each time. The combination function would be an OR function. At the first yes instance of $B$, we return the value as the answer for $A$. If there is no independent set in $G$, the worst number of calls to $O_B$ is $\ell$.</p><hr><h3 id=cook-reductions--poly-time-turing-reductions>Cook Reductions / Poly-time Turing Reductions<a hidden class=anchor aria-hidden=true href=#cook-reductions--poly-time-turing-reductions>#</a></h3><p>Here, we are allowed a polynomial number of oracle calls and polynomial time for transforming the inputs. These are the most general form of reductions, and the other forms of reductions are restrictions of Cook reductions. In the example graph $G$ used for TT reductions, if we assume the number of vertices of $G$ to be a polynomial, then the reduction $A\subseteq B$ using the same exact process would be a Cook reduction.</p><blockquote><p>$A\subseteq_{m} B \implies A\subseteq_{t} B \implies A\subseteq_{T} B$
where, $\subseteq_{m}$ denotes Karp reductions, $\subseteq_{t}$ denotes Truth Table reductions, and $\subseteq_{T}$ denotes Cook reductions.</p></blockquote><p><em>Aside:</em> From the nature of the examples, we can see that Karp reductions only extend to decision problems (problems with yes/no outputs). In contrast, Cook reductions can accommodate search/relation/optimization problems (problems with a set of outputs).</p><hr><h2 id=basic-reductions-in-computability-theory>Basic reductions in Computability Theory<a hidden class=anchor aria-hidden=true href=#basic-reductions-in-computability-theory>#</a></h2><p>In this section, the reader is assumed to have familiarity with concepts of decidability and undecidability. Let us now proceed with some instances of reductions in computability theory.</p><p>Let $A\subseteq B$, and they are decision problems.</p><ul><li>If $A$ is decidable, what can we say about $B$?</li><li>If $A$ is semi-decidable, what can we say about $B$?</li><li>If $A$ is undecidable, what can we say about $B$?</li></ul><p>We know that $B$ is at least as hard as $A$ since $A\subseteq B$. Therefore in the first case, $B$ may be decidable, semi-decidable, or undecidable. In the second case, $B$ may be semi-decidable, or undecidable. In the third case, $B$ is undecidable.</p><p>The third case is of interest here - To show that a $B$ is undecidable, we must find a reduction from $A$ to $B$, where $A$ is already known to be undecidable.
<strong>Note (Advanced):</strong> The reduction function must itself be a computable function.</p><p>Now again, consider $A\subseteq B$ where they are decision problems. We can conclude the following:</p><ul><li>If $B$ is decidable, $A$ is decidable.</li><li>If $B$ is semi-decidable, $A$ is semi-decidable.</li><li>If $A$ is not decidable, then $B$ is not decidable.</li><li>If $A$ is not semi-decidable, then $B$ is not semi-decidable.</li><li>$A\subseteq B$, then $\bar{A}\subseteq \bar{B}$, where $A$ and $B$ are decidable problems.</li></ul><p>The first two statements are true, as discussed above, while 3 is the contrapositive of the first statement and the fourth statement is the contrapositive of the second statement. For the fifth statement, the proof is as follows:</p><blockquote><p>$x\in \bar{A} \iff x\notin A \iff R(x)\notin B \iff R(x)\in\bar{B}$. Therefore, $x\in \bar{A} \iff R(x)\in\bar{B}$.</p></blockquote><p>Immediately we see the power of formalizing the notion of reductions.</p><hr><h2 id=complexity-theoretic-notions>Complexity-Theoretic notions<a hidden class=anchor aria-hidden=true href=#complexity-theoretic-notions>#</a></h2><blockquote><p><strong>This section is for advanced readers only due to the number of prerequisites involved.</strong></p></blockquote><p>A complexity class is a set of computational problems that can be solved using similar amounts of bounded resources (time, space, circuit depth, number of gates, etc.) on a given computational model (Turing machines, circuits, cellular automaton, etc.).</p><p>The complexity classes $\mathrm{P}$, $\mathrm{NP}$, and $\mathrm{PSPACE}$ are closed under Karp and logspace reductions. The complexity classes $\mathrm{L}$ and $\mathrm{NL}$ are closed only under logspace reductions. Closure has the following semantics - given a decision problem $A$ in a complexity class $C$, any problem $B$ such that $B\subseteq A$ is also in $C$.</p><p>We now explore two interesting notions in complexity theory that arise from reductions.</p><h2 id=completeness>Completeness<a hidden class=anchor aria-hidden=true href=#completeness>#</a></h2><p>For a bounded complexity class,</p><blockquote><p>Complete problems are the hardest problems inside their respective complexity class.</p></blockquote><p>A more formal definition of completeness is as follows:</p><blockquote><p>Given a complexity class $C$ which is closed under reduction $r$, if there exists a problem $A$ in $C$ such that all other problems in $C$ are $r$-reducible to $A$, $A$ is said to be C-complete.</p></blockquote><p>For example, an NP-complete problem is in NP, and all problems in NP are Karp-reducible to it. The notions of PSPACE-completeness and EXPTIME-completeness are similarly defined under Karp-reductions.</p><p>The role of reductions in this context can be understood through P-completeness. Consider any non-trivial decision problem in P (trivial problems are akin to constant functions). Every other non-trivial decision problem is Karp-reducible to it. Therefore every non-trivial decision problem is P-complete under Karp-reductions. This definition is essentially the same as P (minus the empty language and $\Sigma^*$).</p><p>Therefore, we come to the following conclusion:</p><blockquote><p>Under Karp-reductions, the notion of P-completeness is <em>semantically useless</em>.</p></blockquote><p>Hence, we use weaker forms of reductions such as logspace reductions or reductions using constant depth circuit computable functions to achieve a more meaningful notion of P-completeness <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><h2 id=self-reducibility>Self reducibility<a hidden class=anchor aria-hidden=true href=#self-reducibility>#</a></h2><p>Decision problems are yes/no problems where we ask if a solution exists. However, sometimes we also want to solve the corresponding search problem to find a solution (if one exists).</p><p>In the context of languages in NP, self-reducibility essentially states that if we can efficiently solve the decision version of a problem, we can also efficiently solve the search/optimization version of the problem. A more formal definition would be as follows:</p><blockquote><p>The search version of a problem Cook-reduces (polynomial-time Turing-reduces) to the decision version of the problem.</p></blockquote><p><strong>Fact:</strong> Every NP-complete problem is self-reducible and <em>downward</em> self-reducible<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This write-up aims to demystify a core technique used in theoretical computer science and provide a few contexts for its usage. For a more rigorous and formal introduction to this topic, please check out Michael Sipser&rsquo;s excellent book &ldquo;An Introduction to the Theory of Computation.&rdquo; There are many more things to say about reductions, and this article barely scratches the surface on any of the topics it touches. However, I feel it would be prudent to stop at this point before it becomes any more of a rambling mess than it already is.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Formally, we consider problems $O$ and $N$ to be <em>languages</em> or sets of strings corresponding to decision problems, i.e., YES/NO problems.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>We will look at the notion of <em>randomized</em> reductions in another post.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>We will look at weaker notions of reductions in another post.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>We will look at the notions of self-reducibility and downward self-reducibility <a href=https://theoretickles.netlify.app/posts/selfreductions/ rel=noopener class=external-link target=_blank>in another post</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><h3 class=see-also>Related posts</h3><div class=related><ul><li><a href=/posts/selfreductions/ target=_blank>Self-Reducibility</a> <span class=related-date>Ôºà2025-02-01Ôºâ</span></li></ul></div><ul class=post-tags><li><a href=https://theoretickles.netlify.app/tags/reductions/>Reductions</a></li><li><a href=https://theoretickles.netlify.app/tags/p-np/>P-Np</a></li><li><a href=https://theoretickles.netlify.app/tags/sat/>SAT</a></li><li><a href=https://theoretickles.netlify.app/tags/halting-problem/>Halting Problem</a></li><li><a href=https://theoretickles.netlify.app/tags/undecidability/>Undecidability</a></li></ul><nav class=paginav><a class=prev href=https://theoretickles.netlify.app/posts/selfreductions/><span class=title>Next ¬ª</span><br><span>Self-Reducibility</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on x" href="https://x.com/intent/tweet/?text=An%20Introduction%20to%20Reductions&amp;url=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f&amp;hashtags=reductions%2cp-np%2cSAT%2chaltingproblem%2cundecidability"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f&amp;title=An%20Introduction%20to%20Reductions&amp;summary=An%20Introduction%20to%20Reductions&amp;source=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f&title=An%20Introduction%20to%20Reductions"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on whatsapp" href="https://api.whatsapp.com/send?text=An%20Introduction%20to%20Reductions%20-%20https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on telegram" href="https://telegram.me/share/url?text=An%20Introduction%20to%20Reductions&amp;url=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Reductions on ycombinator" href="https://news.ycombinator.com/submitlink?t=An%20Introduction%20to%20Reductions&u=https%3a%2f%2ftheoretickles.netlify.app%2fposts%2freductions%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://theoretickles.netlify.app/>Theoretickles</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>