[{"content":"In this post, we discuss an important class of algebraic structures known as $q$-ary lattices that are central to lattice-based cryptographic primitives.\nHardness of problems Computational hardness usually revolves around problems with worst-case hardness guarantees since we want to design algorithms that run efficiently even on the worst possible input.\nOn the other hand, cryptographic schemes require security guarantees for random keys. Therefore, cryptographic applications require problems with average-case hardness guarantees.\nHowever, it is not immediately apparent how to create hard instances of problems with worst-case hardness guarantees. In other words, how to design the most secure keys for cryptographic applications is unclear.\nAny random instance of a problem with average-case hardness guarantee is hard with a positive probability. Suppose one could show that such a random instance is as hard as the hardest instance (a reduction from average-case hardness to worst-case hardness). In that case, such a reduction immediately lends itself to applications in cryptography. This reduction is precisely the central thesis of the seminal work Ajtai'96 1.\nLattice-based Cryptography What is a Lattice? A Lattice $\\Lambda$ or $\\mathcal{L}$ is defined as the set of all integer linear combinations of $n$ linearly independent $m$-dimensional vectors $B=\\{\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}\\}$. Formally, we denote lattices as $$\\mathcal{L}(B)=\\left\\{Bx, x\\in\\mathbb{Z}^{m}\\right\\}=\\left\\{\\sum_{i=1}^{n}x_i\\mathrm{b_i} : x_i\\in\\mathbb{Z}\\right\\}.$$\nThe set $B\\in\\mathbb{R}^{n\\times m}$ is said to be the basis of the lattice $\\Lambda$. We note here that any lattice $\\Lambda$ is characterized by its basis. Shortly, we will generalize the definition of lattices by showing a different characterization of lattices which will be useful in cryptographic construction schemes.\nWhy do we need Lattice-based Cryptography? Ajtai'96 1 showed reductions from lattice based problems with average-case hardness guarantees to lattice problems with worst-case guarantees.\nLet $P$ be the class of lattice based problems defined in Ajtai'96 1. Then, the above reduction implies that any randomly sampled instance of $P$ is as computationally hard to solve as the hardest instance in $P$.\nFor reasons discussed above, this immediately makes lattices an extremely important tool in cryptography. In fact, lattice based cryptographic constructions are invaluable for their many potential advantages as follows:\nLattice-based schemes usually only require linear operations on integers which leads to asymptotic efficiency. Lattice-based schemes have been shown to be resistant to cryptanalysis by quantum algorithms unlike current classical cryptographic schemes which are based on factoring or discrete log (Shor'95 2). This makes lattice-based cryptography the cornerstone of post-quantum cryptography. As noted earlier, random instances of lattice based constructions are “as hard as possible”, which lends itself to conceptual simplicity while designing cryptographic schemes. Hard Lattice problems w/ known worst-case hardness guarantees In this section, we discuss a few lattice problems with known worst-case hardness guarantees.\nShortest Vector Problem $\\left(\\mathrm{SVP}_\\gamma\\right)$ In the $\\gamma$-approximate Shortest Vector Problem, we are asked find the length of the shortest non-zero vector (denoted by $\\lambda_1$) in an $n$-dimensional lattice, approximately, up to a polynomial factor $\\gamma$.\nIt is known that the approximate (and also decision) SVP is NP-hard under a randomized reduction.3 Shortest Independent Vector Problem $\\left(\\mathrm{SIVP}_\\gamma\\right)$ The goal of the $\\gamma$-approximate Shortest Independent Vector problem is to output a set of $n$ linearly independent lattice vectors in an $n$-dimensional lattice, approximately of length$\\leq\\gamma\\lambda_n$.\nSIVP is NP-hard to approximate for any constant approximation factor.4 Shortest Basis Problem $\\left(\\mathrm{SBP}_\\gamma\\right)$ The $\\gamma$-approximate Shortest Basis Problem asks us to find a basis $B={\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}}$ for an $n$-dimensional lattice $\\mathcal{L}(B)$ such that $\\mathrm{max}_i \\lVert\\mathrm{b_i}\\rVert$ is the smallest possible upto a factor of $\\gamma$.\nLater in this post we will see the connection between the above problems.\nConstructing random instances of a Hard Lattice problem In this section, we show a reduction from randomly generated lattices to instances of one of these problems. We now state some important results from Ajtai'961 and Ajtai'995 which deal with the construction of random instances of hard lattice problems. The following lemma shows that hardness of SBP by reduction to SVP.\nLemma 1 (Ajtai'96) If $\\mathrm{SBP}_\\gamma$ has no polynomial time solution for $\\gamma=\\mathrm{poly}(n)$, then we can generate a random lattice $\\Lambda$ (over some distribution $\\mathcal{D}$) together with a \u0026ldquo;short\u0026rdquo; vector $x\\in\\Lambda$ in polynomial time s.t. there is no algorithm which can find a vector shorter than $\\sqrt{n}$ in $\\Lambda$ over $\\mathcal{D}$ w.p. greater than $n^{-c}$, for sufficiently large $n$ and any $c\u0026gt;0$.\nNote: Here \u0026ldquo;short\u0026rsquo;\u0026rsquo; refers to $|x|\\leq\\sqrt{n}$. Later Ajtai'99 5 extended Lemma 1 to the following theorem:\nTheorem 2 (Main Theorem, Ajtai'99) If $\\mathrm{SBP}_\\gamma$ has no polynomial time solution for $\\gamma=\\mathrm{poly}(n)$, then we can generate a random lattice $\\Lambda$ (from the same distribution $\\mathcal{D}$ as in Lemma 1 together with a \u0026ldquo;short\u0026rdquo; basis $B$ of $\\Lambda$ in polynomial time s.t. there is no algorithm which can find a vector shorter than $\\sqrt{n}$ in $\\Lambda$ over $\\mathcal{D}$ w.p. greater than $n^{-c}$, for sufficiently large $n$ and any $c\u0026gt;0$.\nAs we see, Theorem 2 clearly gives us a way to construct a random instance of a hard problem ($\\mathrm{SBP}_\\gamma$). A short basis means $\\lVert \\mathrm{b_i} \\rVert\\leq n\\sqrt{n},i\\in[n]$, where $B=\\{\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}\\}$. In the next, we concretely define the class of \u0026ldquo;random\u0026rdquo; lattices required by the above results.\nThe SIS problem In this section, we take a deeper dive into the special class of random lattices given by Ajtai'961, where it was shown how to\nConstruct a family of one-way functions (based on the earlier family of random lattices as seen in Lemma 1), s.t., If we invert any function from this family of functions with non-negligible probability, it implies that we can solve any instance of the $\\mathrm{SVP}_\\gamma$ for $\\gamma=O({n^c}),c\u0026gt;1$. Therefore, by the previous discussion, we see that the above construction implies an average-case to worst-case reduction. GGH116 later extended the analysis of Ajtai96 to show that the family of one-way functions is also collision-resistant, which is a stronger and more useful property for cryptographic applications.\nWe now define the SIS problem and state a lemma (without proof) of its hardness.\nShort Integer Solution ($\\mathrm{SIS}_{n,m,q,\\beta}$) Let $A$ be a uniformly random matrix , i.e. , $A=\\{a_0| a_1 | \\ldots | a_{m-1}\\}$ consists of $m$ uniformly random vectors $a_i\\in\\mathbb{Z}^n_q,i\\in[m]$. Find a nontrivial $x\\in\\mathbb{Z}^m$, s.t. $|x|\\leq\\beta$ and $Ax\\equiv 0\\mod{q}$.\nHere $A\\sim\\mathbb{Z}_{q}^{n\\times m}$ where $\\mathbb{Z}^n_q$ is the set of all $n$-dimensional integer vectors modulo $q$, i.e., $\\mathbb{Z}^n_q={\\left\\langle x_1,\\ldots, x_n \\right\\rangle}$, where $x_i\\in{0,1,\\ldots,q-1}$ for all $i\\in[n]$.\nWe also note here that without the constraint $|x|\\leq\\beta$, the $\\mathrm{SIS}$ is easy to solve using Gaussian elimination.\nHardness of $\\mathrm{SIS}$ For any $m = \\mathrm{poly}(n)$, any $\\beta \u0026gt; 0$, and any sufficiently large $q \\geq \\beta n^c$ (for any constant $c \u0026gt;0$), solving $\\mathrm{SIS}({n,m,q,\\beta})$ with nonnegligible probability is at least as hard as solving $GapSVP_\\gamma$ and ${SIVP}_\\gamma$ for some $\\gamma = \\beta \\cdot O(\\sqrt{n})$ with a high probability in the worst-case scenario.\nNow we turn our attention to the construction of GGH116 based on the hardness of $\\mathrm{SIS}$.\nA specific CRHF construction (GGH11) We define a Collision-Resistant Hash function (CRHF) as follows:\nSet $m\u0026gt;n\\frac{\\log{q}}{\\log{d}}$. Key: A matrix $A$ chosen uniformly at random from $\\mathbb{Z}^{m\\times n}_q$. Hash function: Define a Shrinking function (shrinking since the domain is greater than the range): $f_A:{0,\\ldots, d-1}^m\\xrightarrow{}\\mathbb{Z}^n_q$ s.t. $f_A(x)=Ax\\mod q$. Goal: Find $x,x^{\\prime}\\in{0,1}^m$ s.t. $f_A(x)=f_A(x^{\\prime})$. Note that finding a solution to the collision problem yields a solution $z=x-x^\\prime$ to the $\\mathrm{SIS}$ problem as $0=f_A(x)-f_A(x^{\\prime})=Ax-Ax^\\prime=Az\\equiv 0\\mod{q}$.\nWe now define a class of random lattices known as $q$-ary lattices and connect it to the $\\mathrm{SIS}$ problem.\n$q$-ary Lattices at last! Given $A\\sim \\mathbb{Z}^{n\\times m}_q$, $q$-ary lattices $\\Lambda_q^\\perp(A)$ are defined as $$\\Lambda_q^\\perp(A)=\\{z\\in\\mathbb{Z}^m:Az=0\\mod{q}\\}.$$\nWe note that the randomness of $\\Lambda_q^\\perp(A)$ stems from the randomness of $A$ itself. $\\Lambda_q^\\perp(A)$ contains all vectors that are orthogonal modulo $q$ to the rows of $A$, i.e. $\\Lambda_q^\\perp(A)$ corresponds to the code whose parity check matrix is $A$.\nHere, we see that we have successfully characterized a lattice in terms of a parity check matrix $A$ instead of a basis. The class of $q$-ary lattices is the class of random lattices required in Lemma 1 and Theorem 2. We end this article with another lemma by Ajtai96 and some properties of $q$-ary lattices which connects the average-case hardness guarantees with worst-case guarantees.\nLemma 2 (Ajtai96) Finding short $z\\in\\Lambda_q^\\perp(A)$ s.t. $\\left|{z}\\right|\\leq \\beta \u0026lt; q$, implies solving $\\mathrm{GapSVP}_{\\beta\\sqrt{n}}$ on any n-dimensional lattice.\nProperties of $q$-ary Lattices A $q$-ary lattice $L$ satisfies $q\\mathbb{Z}^n \\subseteq L \\subseteq \\mathbb{Z}^n$ for $q\\in\\mathbb{Z}$. Any integer lattice $\\mathcal{L}\\subseteq\\mathbb{Z}^n$ is a $q$-ary lattice for some $q$, e.g., whenever q is an integer multiple of the determinant $\\mathrm{det}(\\mathcal{L})$. We are only interested when $q$ is much smaller than $\\left|{\\mathrm{det}(L)}\\right|$. The following two problems are equivalent: Finding a short vector in $\\Lambda_q^\\perp(A)$ s.t. $A\\sim \\mathbb{Z}^{n\\times m}_q$. Finding a short solution to a set of $n$ random equations modulo $q$ in $m$ variables. The dual of any $q$-ary lattice is also a $q$-ary lattice. We represent the dual of a $q$-ary lattice $\\Lambda_q^\\perp(A)$ using the notation $\\Lambda_q(A)$, which is defined as $$\\Lambda_q(A)=\\{z\\in\\mathbb{Z}^m:z=A^Ts\\mod{q},,s\\in\\mathbb{Z}^n\\}.$$ Here $\\Lambda_q(A)$ is the code generated by the rows of $A$, i.e., $A$ itself is the generator matrix of the code. Contrast this with $\\Lambda_q^\\perp(A)$ where $A$ is the parity check matrix. References Ajtai96: M. Ajtai. 1996. Generating hard instances of lattice problems (extended abstract). In Proceedings of the twenty-eighth annual ACM symposium on Theory of Computing (STOC \u0026lsquo;96). Association for Computing Machinery, New York, NY, USA, 99–108. https://doi.org/10.1145/237814.237838\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShor95: Peter W. Shor. 1997. Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer. SIAM J. Comput. 26, 5 (Oct. 1997), 1484–1509. https://doi.org/10.1137/S0097539795293172\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBP23: Huck Bennett and Chris Peikert. Hardness of the (Approximate) Shortest Vector Problem: A Simple Proof via Reed-Solomon Codes. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2023). Leibniz International Proceedings in Informatics (LIPIcs), Volume 275, pp. 37:1-37:20, Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2023). https://doi.org/10.4230/LIPIcs.APPROX/RANDOM.2023.37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBS99: Johannes Blömer and Jean-Pierre Seifert. On the complexity of computing short linearly independent vectors and short bases in a lattice. In Proceedings of the Thirty-first Annual ACM Symposium on Theory of Computing, STOC ’99, pages 711–720, New York, NY, USA, 1999. ACM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAjtai99: Miklós Ajtai. 1999. Generating Hard Instances of the Short Basis Problem. In Proceedings of the 26th International Colloquium on Automata, Languages and Programming (ICAL \u0026lsquo;99). Springer-Verlag, Berlin, Heidelberg, 1–9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGGH11: O. Goldreich, S. Goldwasser, and S. Halevi. Collision-free hashing from lattice problems. Studies in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and Computation: In Collaboration with Lidor Avigad, Mihir Bellare, Zvika Brakerski, Shafi Goldwasser, Shai Halevi, Tali Kaufman, Leonid Levin, Noam Nisan, Dana Ron, Madhu Sudan, Luca Trevisan, Salil Vadhan, Avi Wigderson, David Zuckerman, pages 30–39, 2011\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://theoretickles.netlify.app/posts/qary/","summary":"\u003cp\u003eIn this post, we discuss an important class of algebraic structures known as $q$-ary lattices that are central to lattice-based cryptographic primitives.\u003c/p\u003e\n\u003ch2 id=\"hardness-of-problems\"\u003eHardness of problems\u003c/h2\u003e\n\u003cp\u003eComputational hardness usually revolves around problems with \u003cstrong\u003eworst-case hardness\u003c/strong\u003e guarantees since we want to design algorithms that run efficiently even on the worst possible input.\u003c/p\u003e\n\u003cp\u003eOn the other hand, cryptographic schemes require security guarantees for random keys. Therefore, cryptographic applications require problems with \u003cstrong\u003eaverage-case hardness\u003c/strong\u003e guarantees.\u003c/p\u003e","title":"$q$-ary Lattices"},{"content":" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let\u0026rsquo;s dive in.\nIntroduction In an earlier post, we familiarised ourselves with the notion of reductions. Towards the end, we introduced the notion of self-reducibility which is our main topic of focus today. We start by familiarising ourselves with a few concepts.\nPreliminaries Readers familiar with concepts such as Decision problems, Search Problems, and complexity classes TFNP, PPAD, PLS, etc. can skip this section.\nLanguages: In the world of complexity and computability, a language is a set of strings formed out of some alphabet. Formally, $L\\subseteq\\Sigma^{*}$, where the alphabet $\\Sigma$ is a finite set of symbols, and $\\Sigma^{*}$ refers to the Kleene closure of $\\Sigma$. Last time we formalized reductions in terms of Turing Machines. We now explicitly define mapping reductions (equivalent to Karp reductions for TMs) in terms of languages. Let $L_1 \\subseteq \\Sigma_1^{*}$ and $L_2 \\subseteq \\Sigma_2^{*}$ be languages. Recall that $L_1\\leq_m L_2$ (or, $L_1$ reduces to $L_2$), if there exists a computable function $f: \\Sigma_1^{*}\\mapsto\\Sigma_2^{*}$ s.t. for every $w\\in\\Sigma_1^{*}$, $w\\in L_1\\iff f(w)\\in L_2$. We sometimes use the notation $A\\leq^p_m B$ to denote that the function $f$ is polynomial time constructible.\nDecision Problems: A decision problem is a Boolean-valued function $f:\\Sigma^{*}\\mapsto\\{0,1\\}$. Every language $L\\subseteq\\Sigma^{*}$ can be uniquely associated with a unique decision problem called the membership problem. Here, $x\\in L\\iff f(x)=1$. A Turing machine $T$ computes/solves the decision problem $f$ if for any input $x\\in\\Sigma^{*}$, $T$ halts on any input $x$ and produces output $T(x)=f(x)$.\nHence problems themselves can be construed as languages.\nComplexity classes P and NP are defined w.r.t. decision problems.\nSearch Problems: A search problem is a relation $R\\subset\\Sigma_{in}^{*}\\times\\Sigma_{out}^{*}$, i.e., $(x,y)\\in R$, where $x\\in\\Sigma_{in},y\\in\\Sigma_{out}$ are strings belonging to the input and output alphabets respectively. Search problems are also known as relational problems / optimization problems. $R\\subset\\Sigma_{in}^{*}\\times\\Sigma_{out}^{*}$ is a polynomially-balanced relation if for any $(x,y)\\in R$, $|y|=\\text{poly}(|x|)$. A Turing machine $T$ computes/solves $R$, if for any input $x\\in\\Sigma_{in}$, $T(x)$ halts and produces $y\\in\\Sigma_{out}$ s.t. $(x,y)\\in R$. Since the complexity class NP is defined w.r.t. decision problems, we need to introduce an equivalent notion for search problems. Informally, this is denoted by the class FNP (or Function NP). Formally, a polynomially-balanced relation $R$ defines a NP search problem if $R$ is polynomial-time computable.\nComplexity class TFNP: The complexity class TFNP consists of all search problems in FNP that are total in the sense that a solution is guaranteed to exist. In other words, $R\\in$TFNP$\\iff R\\in$FNP and $R$ is total, i.e., $\\forall x\\in\\Sigma_{in}, \\exists y\\in\\Sigma_{out}$ s.t. $(x,y)\\in R$. It is straightforward to show that TFNP = FNP $\\cap$ co-FNP. It is not believed that TFNP has complete problems since it is a semantic class 1, and various syntactic subclasses have been used to classify the many diverse problems that belong to TFNP. The syntactic subclasses are defined based on the combinatorial principle used to argue totality in TFNP.\nEach subclass of TFNP has a corresponding existence proof principle (for example, an instance of a circuit or graph), one that when applied in a general context, does not yield a polynomial-time algorithm. We define two important subclasses of TFNP below (along with a whole host of others in the footnotes section.)\nComplexity class PLS: Also known as Polynomial Local Search, this is a subclass of TFNP function problems which contains problems that are guaranteed to have a solution because of the lemma that \u0026ldquo;every finite directed acyclic graph has a sink.\u0026rdquo; PLS captures problems of finding a local minimum of an objective function $f$, in contexts where any candidate solution $x$ has a local neighbourhood within which we can readily check for the existence of some other point having a lower value of $f$. One of the motivations behind defining PLS was to capture the notion of local optimum in NP-hard problems like TSP.\nComplexity class PPAD: PPAD is defined as the set of functions in TFNP that reduce in polynomial time to the End-Of-Line (EOL) problem.2 A problem is complete for PPAD if it belongs to PPAD and if EOL reduces in polynomial time to that problem. PPAD is contained in PPA $\\cap$ PPP 34 and could be regarded as the directed version of the class PPA.3\nSome interesting search/optimization problems that belong to the classes PPAD and PLS are described below.\nThe problem of finding a Nash equilibrium (NASH) in a normal form game of two or more players with specified utilities, is in PPAD. NASH with three players is PPAD-complete. Computing fixed-points in Brouwer functions5 is in PPAD. Gradient Descent is in CLS = PPAD $\\cap$ PLS.67 Search to Decision Reductions We note here that Decision Problems answer the following flavour of questions:\nGiven a problem $P$, is $x$ a solution to $P$? (Yes/No).\nOn the other hand, Search Problems answer the following flavour of questions:\nGiven a problem $P$, output a solution to $P$ with some property.\nFor example, given a problem $P$, output a solution to $P$ that has the minimum length. We use the satisfiability problem (SAT) as an example to further illustrate the two notions:\nDecision problem: Given a propositional formula $\\phi$, decide if $\\phi$ is satisfiable. Search problem: Given a propositional formula $\\phi$, find a satisfying assignment for $\\phi$. Note that SATSearch $\\in$ FNP but SATSearch $\\notin$ TFNP, since a formula may be unsatisfiable.\nAs we see above, if it is easy to solve the Search version of a problem $P$, it is straightforward to solve the Decision version of $P$. The more challenging question is:\nCan we efficiently solve the Search version of a problem $P$, if we know how to solve the Decision version of $P$ efficiently?\nFormally, let $O_D^p$ be a decision oracle for a search problem $R\\subset\\Sigma_{in}^{*}\\times\\Sigma_{out}^{*}$ s.t. querying $O_D^p$ produces $\\mathbb{I}[ \\exists x\\in\\Sigma_{in};|; x \\text{ has property } p]$; i.e., querying $O_D$ with an appropriate parameter for a property $p$ outputs a yes or a no indicating if there exists any input that satisfies the property $p$ (usually taken to be some bound on the input size). Our goal now is to produce $y\\in\\Sigma_{out}$ s.t. $(x,y)\\in R$, using oracle calls to $O_D^p$.\nExamples of Search-to-Decision reduction In this section, we see some examples of search-to-decision reductions. Let us start with designing a search-to-decision reduction for SAT, which is an NP-complete problem.\nThere are two inputs to the SATSearchToDecision() reduction\nthe propositional formula $\\phi$ or f, and the decision oracle for SAT on $O_D$ or DSAT(f,assign) which takes as input a propositional formula f and a restricted assignment assign and returns yes iff f is satisfiable under the restriction assign. The output of the SATSearchToDecision() procedure is a satisfying assignment for $\\phi$( or f). SATSearchToDecision(f,DSAT()){ assignarr = [*,*,....,*];// Intitalize assignarr as an n-bit empty array. if(DSAT(f,assignarr)=0) // is f satisfiable without restrictions? return -1; // f is not satisfiable for (i=1;i\u0026lt;=n; i++){ assignarr[i]= 0 //Fix the ith bit in x to be 1. // This fixes the ith literal in f. if (DSAT(f,assignarr)==1) continue; // move on to the i+1th coordinate, // with the ith bit set to 0. assignarr[i]= 1 //Fix the ith bit in x to be 0. // This fixes the ith literal in f. if (DSAT(f,assignarr)==1) continue; // move on to the i+1th coordinate, // with the ith bit set to 1. return assignarr; } Earlier we saw that the property used by the decision oracle was a restricted assignment. We list another example of a search-to-decision reduction for the Clique problem (another one of Karp\u0026rsquo;s original 21 NP-complete problems), to give a flavour of a different decision oracle property - based on size.\nThe Clique Decision problem: Given a graph $G=(V,E)$, decide if $G$ contains a clique of size $\\leq k$. The Clique Search problem: Given a graph $G=(V,E)$, find a clique of size $\\leq k$ in $G$ if it exists. As seen above, there are two inputs to the CliqueSearchToDecision() reduction:\nthe graph $G$ as an adjacency list L, and the decision oracle for Clique on $O_D$ or DCLIQUE(L,k) which takes as input a adjacency list L and a parameter k and returns yes iff the graph corresponding to L contains a clique of size at most k. The output of the CliqueSearchToDecision() procedure is an adjacency list corresponding to a clique in $G$ of size $\\leq k$. Recall that an adjacency list is a collection of unordered lists used to represent a finite graph. We use the following definition of adjacency lists (this is a modification of the definition given in CLRS): An adjacency list is a singly linked list where each element in the list corresponds to a particular vertex, and each element in the list itself points to a singly linked list of the neighboring vertices of that vertex. See the diagram below.\nSATSearchToDecision(L,DCLIQUE()){ if(DCLIQUE(L,k)=0) return -1; // There is no clique of size at most k for every vertex v of G { Let Lv be the new adjacency list obtained by removing vertex v from G. // easily done using the above datastructure. if(DCLIQUE(Lv,k)=1) L = Lv; // update the graph to reflect G = G-v. } return L; } Above we saw examples of problems where both search and decision verions are hard. There exist problems where both the search and decision versions are easy, for example, Maximum Matching, Shortest Path, etc. A typical example of a problem where the decision version is easy and while the search version is supposedly hard is as follows:\nThe Decision problem (Primality testing): Given a natural number $n$, decide if $n$ is prime. The Search problem (Factoring): Given (the binary representation of) a natural number $n$, produce all of its factors. Note that unlike SATSearch and CliqueSearch, Factoring$\\in$TFNP since any natural number will always have at least two factors.\nMany important classes of cryptosystems such as RSA 8 depend on the hardness of the Factoring Search problem.9 The Factoring Decision problem is also known as the Primality testing problem.10\nFormal notion of Self-reducibility We now move on to the main topic of this post - the notion of self-reducibility (also known as auto-reducibility). A problem is self-reducible if it admits an efficient search-to-decision reduction, i.e., any efficient solution to the decision version of the problem implies an efficient solution to the search version of the problem.\nNote: Not every problem in NP is necessarily self-reducible. If EE$\\neq$NEE, then there exists a language in NP that is not self-reducible (Bellare and Goldwasser'94).11\nDownward self-reducibility A search problem $R$ is downward self-reducible (d.s.r) if there is a polynomial time oracle algorithm for $R$ that on input $x \\in \\Sigma^{*}$ makes queries to an $R$-oracle of size strictly less than $|x|$. In other words, a language $L$ is d.s.r. if there exists a polynomial time algorithm $A^O$ deciding $x\\overset{?}{\\in} L$ with a membership oracle $O$ for $L$ that can handle subqueries for strings $z\\overset{?}{\\in} L$ s.t. $|z|\u0026lt;|x|$.\nWe can extend the notion of downward self-reducibility to functions or decision problems as follows: A function $f:\\Sigma^{*}\\mapsto \\{0,1\\}$ is downward self-reducible if there exists a polynomial time algorithm $A^{O_f}$ s.t. on any input of length $n$, $A$ only makes queries of length $\u0026lt;n$ to the membership oracle ${O_f}$, and for every input $x$, $A^{O_f}(x)=f(x)$.\nIt is easy to see that SAT is d.s.r. since given any formula $\\phi$ on $n$-variables, one can consider only querying on restrictions of $\\phi$ to figure out if $\\phi$ is satisfiable.\nAll NP-complete decision problems are downward self-reducible.\nThis is straightforward to show using the fact that there exists a Karp-reduction from SAT to $L$ for any NP-complete problem $L$.\nEvery downward self-reducible decision problem is in PSPACE.\nProof. Let the input to a d.s.r. problem $f$ be $x$. Then any algorithm $A$ that solves $f$ will make queries to some oracle and recursively compute the answer to each query. The depth of the recursion is at most |x|, and at each level of recursion, the algorithm needs to remember the state which requires space at most poly(|x|). This last point holds because the basic computation runs in polynomial time, and hence polynomial space.\nNote: We also recall that PSPACE is closed 12 under Karp-reductions. Since, all d.s.r. languages belong to PSPACE, we can conclude that PSPACE is hard (worst-case or average-case) iff a d.s.r. language is hard (in the same sense.)\nEvery downward self-reducible search problem in TFNP is in PLS. 13\nHence PLS is in some senses the functional analogue of PSPACE. Harsha et al. (2023) 13 also show that most natural PLS-complete problems are downward self-reducible. We end with an important open question by Harsha et al. (2023) 13:\nIs Factoring downward self reducible?\nIf Factoring is downward self-reducible, then Factoring$\\in$UEOPL$\\subseteq$PPAD$\\cap$PLS 13. The complexity class UniqueEOPL (Unique End of Potential Line) captures search problems with the property that their solution space forms an exponentially large line with increasing cost.14 From one candidate solution we can calculate another candidate solution in polynomial time. The end of that line is the (unique) solution of the search problem. This implies that no efficient factoring algorithm exists using the factorization of smaller numbers.\nAn application of Downward Self-Reductions: Mahaney\u0026rsquo;s Theorem A language $L$ is (polynomially) sparse if it the number of strings of length $n$ in $L$ is bounded by a polynomial in $n$.\n[Mahaney\u0026rsquo;s Theorem] Assuming P $\\neq$ NP, there are no sparse NP-complete languages.\nProof Sketch: Recall the d.s.r tree of SAT. Given a SAT formula $\\phi[x_1,\\ldots,x_n]$ at the first level we can restrict the formula to $\\phi_0[0,\\ldots,x_n]$ and $\\phi_1[1,\\ldots,x_n]$. If $\\phi$ is satisfiable, then at least one of $\\phi_0$ or $\\phi_1$ is satisfiable. Hence, at the $\\ell$th level, at least one of the $2^{\\ell}$ formulas have to be satisfiable for the original formula to be satisifable. If $L$ is a sparse NP-complete language, we have $SAT\\leq^p_m L$. Hence, using a mapping reduction from SAT to $L$, we can prune the d.s.r. tree s.t. at the $\\ell$th level to only contend with $\\text{poly}(\\ell)$ formulas. This straightforwarly yields a polynomial time SAT algorithm, since there are only $n$ levels. This violates the Exponential Time Hypothesis, and therefore there does not exist any $L$ that is both sparse and NP-complete.\nThe above proof sketch is due to Joshua Grochow15, who credits Manindra Agrawal for the original idea.\nWhat\u0026rsquo;s Next? We have looked at self-reducibility and downward self-reducibility. In follow-up posts, we will introduce the notions of randomized reductions, random self-reducibility, and pseudorandom self-reducibility.\nA complexity class is called a semantic class if the Turing Machine (TM) defining this class has a property that is undecidable. See Papadimitrou\u0026rsquo;s original paper and this Stackexchange link for a more formal discussion on this topic. In a nutshell, promise classes such as RP, ZPP, BPP are semantic.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEOL is the class of problems which can be reduced to the EOL problem instance: Given a exponentially large directed graph consisting of lines and cycles on the vertex set $[2^𝑛]$, find any sink of the graph assuming vertex 1 is the source and every vertex has in and out degree at most 1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nComplexity class PPA: Also known as Polynomial Parity Argument, this class captures computational problems whose totality is rooted in the handshaking lemma for undirected graph: \u0026ldquo;all graphs of maximum degree 2 have an even number of leaves.\u0026rdquo; More precisely, PPA captures search problems for which there is a polynomial-time algorithm that, given any string, computes its \u0026rsquo;neighbor\u0026rsquo; strings (of which there are at most two). Then given a leaf string (i.e. one with only one neighbor), the problem is to output another leaf string.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe existence of solutions for problems in complexity class PPP: (also known as Polynomial Pigeonhole Principle) is guaranteed by the pigeonhole principle: if $n$ balls are placed in $m \u0026lt; n$ bins then at least one bin must contain more than one ball.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBrouwer\u0026rsquo;s fixed-point theorem states that for any continuous function $f$ mapping a nonempty compact convex set to itself, there is a point $x_0$ such that $f(x_0)=x_0$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFinding a point where Gradient Descent terminates is equivalent to finding a KKT point—when the domain is bounded. Computing a KKT point of a continuously differentiable function over $[0, 1]^2$ is PPAD $\\cap$ PLS-complete.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe EOL class is a combinatorially defined alternative to the complexity class CLS (Continuous Local Search), which contains Gradient Descent and various fixed point problems. CLS is the smallest known subclass of TFNP not known to be in P, and hardness results for it imply hardness results for PPAD and PLS simultaneously.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe RSA problem: Find $M$ given the public key $(n,e)$ and a cipher text $C\\equiv M^e\\mod n$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf we can solve the factoring problem then we can solve the RSA problem by factoring the modulus n. Hence, Factoring $\\implies$ RSA.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe seminal result of Agarwal, Kayal, and Saxena showed that this problem is in P. See this writeup for more details on the result.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEE equals DTIME($2^{2^{O(n)}}$).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe recall the notion of notion of complete problems here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPrahladh Harsha, Daniel Mitropolsky, and Alon Rosen. Downward Self-Reducibility in TFNP. ITCS 2023. DOI.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOne interesting subclass of EOL is End-Of-Potential-Line (EOPL) where in addition to the EOL problem setup, we are also given a potential function that increases along each edge. It is known that EOPL = PPAD $\\cap$ PLS-complete. If this graph has a unique sink, then the complexity class is known as UEOPL. It is an open question if UEOPL $\\overset{?}{=}$ PPAD $\\cap$ PLS-complete.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee this preprint.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://theoretickles.netlify.app/posts/selfreductions/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eThis post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let\u0026rsquo;s dive in.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn an earlier post, we familiarised ourselves with the \u003ca href=\"https://theoretickles.netlify.app/posts/reductions\"\u003enotion of reductions\u003c/a\u003e. Towards the end, we introduced the notion of \u003cstrong\u003eself-reducibility\u003c/strong\u003e which is our main topic of focus today. We start by familiarising ourselves with a few concepts.\u003c/p\u003e","title":"Self-Reducibility"},{"content":" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let\u0026rsquo;s dive in.\nIntroduction The What and Why of reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs. Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\nMost unsolved problems we encounter are already related to some existing problem we know about.\nThis relation to solved problems is something that we can now prod and prick incessantly and eventually use to arrive at a conclusion about new problems. While hardly elegant in its conception, this approach is highly sophisticated in its execution. This is the basis for the mathematical technique known as reduction.\nWhen can we use reduction techniques? First we start by describing two types of problems: hard and easy! Hard problems are typically those that the problem solver does not know how to solve or knows are very hard to solve, while easy problems are simply easy to solve.\nSince the notion of hardness as described above is quite subjective and non-rigorous, we formalize it below by quantifying the capabilities of the problem solver.\nThroughout this post, we assume that the notion of hard and easy depends only on the computational resources available to the problem solver.\nAccess to powerful computational models might allow us (the problem solver) to efficiently find solutions to previously hard-to-solve problems. For example, SAT can be easily solved if we have know how to solve the Halting problem. We shall expand on this example later in this post.\nTake two problems1, $O$ (an old problem) and $N$ (a new problem). Suppose we make the astute observation that the underlying mathematical structure of $N$ is similar to $O$. Therefore, instead of trying to solve $N$ from scratch, we can hope to somehow use $O$ to solve $N$. Consider the following situations of interest that might arise where we would ideally like to avoid reinventing the wheel.\nCase 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve In this case, mathematically, we say that $N\\subseteq O$. In this case, our primary approach is to transform instances of $N$ into instances of $O$ and then use the algorithm for $O$ to solve for $N$. In other words, if we can demonstrate that $N$ is a special case of $O$, we solve $N$ using our existing knowledge of $O$.\nReducing problem $A$ to $B$ is the process of transforming instances of $A$ into instances of $B$. Note that this is only possible if $A\\subseteq B$.\nCase 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve In this case, we can again exploit the structural similarities between $O$ and $N$, but this time to a different end. Without prior context, it is difficult to show that $N$ is hard to solve. However, using the hardness of $O$, it is easy to show that $N$ is hard to solve as well.\nWe start with the assumption that $N$ is easy to solve, i.e., there exists some algorithm to efficiently decide membership in the set $N$. We use this assumption to arrive at a contradiction. Now exploiting the similarities between $O$ and $N$, we transform all instances of $O$ to some instance of $N$.\nSince $O$ reduces to $N$, i.e., there is a way to efficiently decide membership in $O$ using the algorithm for $N$. However, we know for sure that $O$ is hard to solve. This leads us to a contradiction, which is resolved by removing our assumption that $N$ is easy to solve. Therefore $N$ has to be hard to solve as well.\nLet us explore a few properties of reductions now.\nRelative Hardness For any pair of languages / decision problems $A$ and $B$, if $A\\subseteq B$, then $B$ is at least as hard as $A$ to solve, i.e., $A$ cannot be harder to solve than $B$.\nReductions as relations Reflexivity of reductions: Trivially $A\\subseteq A$ by using identity transformations. Transitivity of reductions: If $A\\subseteq B$ and $B\\subseteq C$ then we can prove that $A\\subseteq C$ by composing the transformations. Reductions are, however, not symmetric relations (and, therefore, by extension, not equivalence relations). $A\\subseteq B$ does not imply $B\\subseteq A$. A concrete example of this will be discussed later in detail: We can reduce SAT, which is a decidable problem, to the Halting problem, which is an undecidable problem, but the converse is not true (by definition of decidability).\nAn alternate view of reductions Reductions are a highly general family of techniques, and to provide a rigorous formalization of reductions, we consider some specific variants. An interesting way of looking at reductions is as follows:\nGiven an oracle for a problem $B$, can we solve $A$ efficiently by making oracle calls to $B$? If yes, then $A\\subseteq B$.\nHere, an oracle is nothing but a black-box subroutine for efficiently solving a problem. The beauty of reductions lie in the fact that we do not need to bother about the internal mechanisms of the oracle itself. Nor do we have to worry about construct the oracle itself. We simply have to use the oracle\u0026rsquo;s existence. Implicitly, reduction is a two-step process. Suppose $A\\subseteq B$.\nSuppose there exists a subroutine $R$ that transforms yes instances of $A$ into yes instances of $B$, and no instances of $A$ into no instances of $B$. First, input instances $x$ (that may/may not belong to $A$) are reduced/transformed into possible instances of $B$ using the subroutine $R$. Next, we perform invocation(s) to the oracle for $B$ to decide the membership of $R(x)$ in $B$. Explicitly, we perform the following computation: $O_B(R(x))$. If $O_B(R(x))=1$, then we decide that $x\\in A$. Otherwise, we decide that $x\\notin A$. $$x\\in A \\iff R(x) \\in B.$$ The notion of efficiency is twofold.\nFirstly, we are concerned with how efficient the transformation/reduction from $x$ to $R(x)$ is. The notion of efficiency in transforming $x$ to $R(x)$ has a small caveat:\nOnly the construction of the reduction method $R$ needs to be efficient, and solving $O_B(R(x))$ does not need to be efficient. Sometimes, we might require more than one call to the oracle $O_B$, depending on the problems at hand. In this case, we are concerned with how many times the oracle is invoked.\nWe explore the notion of efficiency in reductions via an example.\nExample of an \u0026ldquo;efficient\u0026rdquo; reduction Consider the following C pseudocode:\nint A(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } Writing this piece of code took finite time, and was certainly efficient. However, depending on the value of the input the for loop will never terminate for values of $x\u0026gt;11$. Therefore, this program may become inefficient during runtime. The notion of efficiency we consider during reductions (unlike computational scenarios) is how efficiently we can write the code for function A, and not how efficiently A transforms $x$ into $R(x)$. A Prototypical Reduction: SAT to the Halting Problem Note: This is a highly technical section, and would need familiarity to the satisfiability problem and the Halting problem. We encourage the reader to familiarize themselves with the formal definitions of these problems first before going through this section. We give an intuitive description of the problems below.\nThe Halting Problem Recall the earlier piece of pseudocode with a slight modification.\nint A1(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } int A2(int x){ for(;x\u0026gt;12;x--);// This loop will always terminate return x; // This is R(x) } Note A2 will always halt no matter the input, while A1 may never halt depending on the input.\nImagine you would like to design a function $B$ that takes the binary description of any single parameter function $A$ and any arbitrary input $x$, and find out whether $A$ will halt on input $x$.\nThe Halting problem (HALT): Given (the binary encoding of) any arbitrary Turing Machine $A$ and an arbitrary input (encoded as a binary string) $x$, does $A(x)$ halt?\nThis in effect describes the Halting problem, where $B$ is a universal Turing machine and $A$ can be any Turing Machine. It has been shown that there does not exist any such $B$ which solves this problem. Therefore the Halting problem cannot be decided, and is formally referred to as an undecidable problem.\nThe SAT Problem A Boolean formula $f$ accepts as input an $n$-bit string and outputs a $1$ (if it accepts the string), or a $0$ (if it rejects the string). Mathematically, $f:{0,1}^{n}\\xrightarrow{}{0,1}$.\nThe satisfiability(SAT) problem: Given a Boolean formula $f$ and an arbitrary $n$ bit string $x\\in{0,1}^{n}$, is $f(x)=1$?\nNote that the structure of SAT is subsumed by the structure of the Halting problem. If the decider to Halting problem ever halts, we can find out whether $f(x)=1$.\nEven though SAT can be quite hard to solve computationally (may have exponential runtime depending on the structure of the formula), we can always construct an algorithm to solve it. Therefore, right off the bat, we observe that SAT is easier to solve (it is decidable) than the Halting problem.\nReducing SAT to HALT Let us finally have a look at how we would reduce SAT to the Halting problem.\nOur input $x$ is a Boolean formula. We want to output if this formula is satisfiable.\nWe construct a TM (Turing Machine) $T$ which accepts $x$ and does the following:\n$T$ iterates over all possible assignments to find a satisfying assignment. 🔴 This may require exponential runtime in the size of the formula.\nIf $T$ finds a satisfying assignment, halt and return 1. 🟣 Hence, if x is satisfiable, T halts. Otherwise, we put $T$ into an infinite loop. 🟣 Hence, T halts iff if x is satisfiable.\nOur reduction $R(x)=\\langle\\langle T\\rangle,x\\rangle$ takes the SAT formula $x$ and returns an encoding of the above Turing machine $T$ coupled with $x$, s.t., yes instances of SAT map to yes instances of HALT, and no instances of SAT map to no instances of HALT. 🟣 Note that $R(x)$ at this point can be compared to a compiled binary which has not yet been executed.\nWe pass $R(x)$ to $O_{\\text{HALT}}$.\nIf $O_{\\text{HALT}}(R(x))$ returns yes, this implies $T$ halts on input $x$, which in turn implies $x$ has a satisfying assignment. Therefore $R(x)\\in\\text{HALT}\\implies x\\in\\text{SAT}$. If $O_{\\text{HALT}}(R(x))$ returns no, then $T$ does not halt on input $x$, which implies that $x$ does not have a satisfying assignment. Therefore $R(x)\\notin\\text{HALT}\\implies x\\notin\\text{SAT}$. We can take the contrapositive of this expression to obtain $x\\in\\text{SAT}\\implies R(x)\\in\\text{HALT}$. Once again we note the following (recall this).\nIn step 2, we are simply constructing the TM $T$, not executing it. Think of this as writing a C or Java program/executable for $T$. However, we are never actually going into runtime; i.e. executing the executable at any point.\nThe reduction is the transformation of the SAT formula $x$ to an encoding of both the Turing Machine and the SAT formula, which is an instance of the Halting problem (HALT$_{TM}$ requires an arbitrary TM and an input string to the TM).\nTaxonomy of Polynomial-time reductions In this post, we only consider polynomial-time deterministic reductions, i.e.,\nthe time taken to transform $x$ to $R(x)$ using a DTM, and the number of calls to $O_B$ to decide membership of $R(x)$ in $B$, are both polynomial. These are the most commonly studied types of reductions,2 and we look at three kinds of polynomial-time reductions.\nKarp Reductions / Many-one reductions These are the most restrictive type of polynomial reductions. Given a single input $x$, $R(x)$ produces a single instance $y$ such that $x\\in A\\iff y\\in B$. Therefore, we have to perform only one oracle call to $O_B$. The earlier reduction from SAT to HALT was a Karp reduction.\nWe can choose to strengthen the notion of Karp reductions to include weaker forms of reductions. For example, in logspace many-one reductions, we can compute $R(x)$ using just logarithmic space instead of polynomial time. Even more restrictive notions of reductions consider reductions computable by constant depth circuit classes.\nTruth Table Reductions These are reductions in which given a single input $x$, $R(x)$ produces a constant number of outputs $y_1,y_2,\\ldots, y_k$ to $B$. The output $O_A(x)$ can be expressed in terms of a function $f$ that combines the outputs $O_B(y_i)$ for $i\\in[1,\\ldots,k]$.\nLet us assume that $f$ outputs $1$ for the desired combination and $0$ otherwise. $$ x\\in A\\iff f\\left(y_1\\in B, y_1\\in B,\\ldots,y_k\\in B\\right)=1 $$ Here, $f$ is efficiently computable, and there are a constant ($k$) number of oracle calls to $O_B$.\nLet us consider an example. Consider two problems on a graph $G$ with a constant number of vertices $\\ell$.\n$A$: What is the minimum sized independent set for $G$? $B$: Does $G$ have an independent set of size $k$?\nThe reduction $A\\subseteq B$ would involve looping from $1$ to $\\ell$ and querying $B$ each time. The combination function would be an OR function. At the first yes instance of $B$, we return the value as the answer for $A$. If there is no independent set in $G$, the worst number of calls to $O_B$ is $\\ell$.\nCook Reductions / Poly-time Turing Reductions Here, we are allowed a polynomial number of oracle calls and polynomial time for transforming the inputs. These are the most general form of reductions, and the other forms of reductions are restrictions of Cook reductions. In the example graph $G$ used for TT reductions, if we assume the number of vertices of $G$ to be a polynomial, then the reduction $A\\subseteq B$ using the same exact process would be a Cook reduction.\n$A\\subseteq_{m} B \\implies A\\subseteq_{t} B \\implies A\\subseteq_{T} B$ where, $\\subseteq_{m}$ denotes Karp reductions, $\\subseteq_{t}$ denotes Truth Table reductions, and $\\subseteq_{T}$ denotes Cook reductions.\nAside: From the nature of the examples, we can see that Karp reductions only extend to decision problems (problems with yes/no outputs). In contrast, Cook reductions can accommodate search/relation/optimization problems (problems with a set of outputs).\nBasic reductions in Computability Theory In this section, the reader is assumed to have familiarity with concepts of decidability and undecidability. Let us now proceed with some instances of reductions in computability theory.\nLet $A\\subseteq B$, and they are decision problems.\nIf $A$ is decidable, what can we say about $B$? If $A$ is semi-decidable, what can we say about $B$? If $A$ is undecidable, what can we say about $B$? We know that $B$ is at least as hard as $A$ since $A\\subseteq B$. Therefore in the first case, $B$ may be decidable, semi-decidable, or undecidable. In the second case, $B$ may be semi-decidable, or undecidable. In the third case, $B$ is undecidable.\nThe third case is of interest here - To show that a $B$ is undecidable, we must find a reduction from $A$ to $B$, where $A$ is already known to be undecidable. Note (Advanced): The reduction function must itself be a computable function.\nNow again, consider $A\\subseteq B$ where they are decision problems. We can conclude the following:\nIf $B$ is decidable, $A$ is decidable. If $B$ is semi-decidable, $A$ is semi-decidable. If $A$ is not decidable, then $B$ is not decidable. If $A$ is not semi-decidable, then $B$ is not semi-decidable. $A\\subseteq B$, then $\\bar{A}\\subseteq \\bar{B}$, where $A$ and $B$ are decidable problems. The first two statements are true, as discussed above, while 3 is the contrapositive of the first statement and the fourth statement is the contrapositive of the second statement. For the fifth statement, the proof is as follows:\n$x\\in \\bar{A} \\iff x\\notin A \\iff R(x)\\notin B \\iff R(x)\\in\\bar{B}$. Therefore, $x\\in \\bar{A} \\iff R(x)\\in\\bar{B}$.\nImmediately we see the power of formalizing the notion of reductions.\nComplexity-Theoretic notions This section is for advanced readers only due to the number of prerequisites involved.\nA complexity class is a set of computational problems that can be solved using similar amounts of bounded resources (time, space, circuit depth, number of gates, etc.) on a given computational model (Turing machines, circuits, cellular automaton, etc.).\nThe complexity classes $\\mathrm{P}$, $\\mathrm{NP}$, and $\\mathrm{PSPACE}$ are closed under Karp and logspace reductions. The complexity classes $\\mathrm{L}$ and $\\mathrm{NL}$ are closed only under logspace reductions. Closure has the following semantics - given a decision problem $A$ in a complexity class $C$, any problem $B$ such that $B\\subseteq A$ is also in $C$.\nWe now explore two interesting notions in complexity theory that arise from reductions.\nCompleteness For a bounded complexity class,\nComplete problems are the hardest problems inside their respective complexity class.\nA more formal definition of completeness is as follows:\nGiven a complexity class $C$ which is closed under reduction $r$, if there exists a problem $A$ in $C$ such that all other problems in $C$ are $r$-reducible to $A$, $A$ is said to be C-complete.\nFor example, an NP-complete problem is in NP, and all problems in NP are Karp-reducible to it. The notions of PSPACE-completeness and EXPTIME-completeness are similarly defined under Karp-reductions.\nThe role of reductions in this context can be understood through P-completeness. Consider any non-trivial decision problem in P (trivial problems are akin to constant functions). Every other non-trivial decision problem is Karp-reducible to it. Therefore every non-trivial decision problem is P-complete under Karp-reductions. This definition is essentially the same as P (minus the empty language and $\\Sigma^*$).\nTherefore, we come to the following conclusion:\nUnder Karp-reductions, the notion of P-completeness is semantically useless.\nHence, we use weaker forms of reductions such as logspace reductions or reductions using constant depth circuit computable functions to achieve a more meaningful notion of P-completeness 3.\nSelf reducibility Decision problems are yes/no problems where we ask if a solution exists. However, sometimes we also want to solve the corresponding search problem to find a solution (if one exists).\nIn the context of languages in NP, self-reducibility essentially states that if we can efficiently solve the decision version of a problem, we can also efficiently solve the search/optimization version of the problem. A more formal definition would be as follows:\nThe search version of a problem Cook-reduces (polynomial-time Turing-reduces) to the decision version of the problem.\nFact: Every NP-complete problem is self-reducible and downward self-reducible4.\nConclusion This write-up aims to demystify a core technique used in theoretical computer science and provide a few contexts for its usage. For a more rigorous and formal introduction to this topic, please check out Michael Sipser\u0026rsquo;s excellent book \u0026ldquo;An Introduction to the Theory of Computation.\u0026rdquo; There are many more things to say about reductions, and this article barely scratches the surface on any of the topics it touches. However, I feel it would be prudent to stop at this point before it becomes any more of a rambling mess than it already is.\nFormally, we consider problems $O$ and $N$ to be languages or sets of strings corresponding to decision problems, i.e., YES/NO problems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe will look at the notion of randomized reductions in another post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe will look at weaker notions of reductions in another post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe will look at the notions of self-reducibility and downward self-reducibility in another post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://theoretickles.netlify.app/posts/reductions/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eThis post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. The reader is referred to the book by Sipser, or the book by Arora and Barak for any formal definitions that have been skipped in this post. Without further ado, let\u0026rsquo;s dive in.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"the-what-and-why-of-reductions\"\u003eThe What and Why of reductions?\u003c/h3\u003e\n\u003cp\u003eFrom Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.\nTherefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\u003c/p\u003e","title":"An Introduction to Reductions"}]