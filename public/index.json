[{"content":"In this post, we discuss an important class of algebraic structures known as $q$-ary lattices that are central to lattice-based cryptographic primitives.\nHardness of problems Computational hardness usually revolves around problems with worst-case hardness guarantees since we want to design algorithms that run efficiently even on the worst possible input.\nOn the other hand, cryptographic schemes require security guarantees for random keys. Therefore, cryptographic applications require problems with average-case hardness guarantees.\nHowever, it is not immediately apparent how to create hard instances of problems with worst-case hardness guarantees. In other words, how to design the most secure keys for cryptographic applications is unclear.\nAny random instance of a problem with average-case hardness guarantee is hard with a positive probability. Suppose one could show that such a random instance is as hard as the hardest instance (a reduction from average-case hardness to worst-case hardness). In that case, such a reduction immediately lends itself to applications in cryptography. This reduction is precisely the central thesis of the seminal work Ajtai'96 1.\nLattice-based Cryptography What is a Lattice? A Lattice $\\Lambda$ or $\\mathcal{L}$ is defined as the set of all integer linear combinations of $n$ linearly independent $m$-dimensional vectors $B=\\{\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}\\}$. Formally, we denote lattices as $$\\mathcal{L}(B)=\\{Bx, x\\in\\mathbb{Z}^{m}\\}=\\left\\{\\sum_{i=1}^{n}x_i\\mathrm{b_i} : x_i\\in\\mathbb{Z}\\right\\}.$$\nThe set $B\\in\\mathbb{R}^{n\\times m}$ is said to be the basis of the lattice $\\Lambda$. We note here that any lattice $\\Lambda$ is characterized by its basis. Shortly, we will generalize the definition of lattices by showing a different characterization of lattices which will be useful in cryptographic construction schemes.\nWhy do we need Lattice-based Cryptography? Ajtai'96 1 showed reductions from lattice based problems with average-case hardness guarantees to lattice problems with worst-case guarantees. Intuitively, this shows that any randomly sampled instance of a lattice is as hard as the hardest instance.\nLater in this post, we will discuss a few lattice problems with known worst-case hardness guarantees. This immediately makes lattices an extremely important tool in cryptography.\nIn fact, lattice based cryptographic constructions are invaluable for their many potential advantages as follows:\nLattice-based schemes usually only require linear operations on integers which leads to asymptotic efficiency. Lattice-based schemes have been shown to be resistant to cryptanalysis by quantum algorithms unlike current classical cryptographic schemes which are based on factoring or discrete log (Shor'95 2). This makes lattice-based cryptography the cornerstone of post-quantum cryptography. As noted earlier, random instances of lattice based constructions are “as hard as possible”, which lends itself to conceptual simplicity while designing cryptographic schemes. Hard Lattice problems w/ known worst-case hardness guarantees In this section, we discuss some important worst-case hard problems associated with lattices. As stated earlier, we would like to show a reduction from randomly generated lattices to instances of one of these problems.\nShortest Vector Problem $\\left(\\mathrm{SVP}_\\gamma\\right)$ In the $\\gamma$-approximate Shortest Vector Problem, we are asked find the length of the shortest non-zero vector (denoted by $\\lambda_1$) in an $n$-dimensional lattice, approximately, up to a polynomial factor $\\gamma$.\nIt is known that the approximate (and also decision) SVP is NP-hard under a randomized reduction.3 Shortest Independent Vector Problem $\\left(\\mathrm{SIVP}_\\gamma\\right)$ The goal of the $\\gamma$-approximate Shortest Independent Vector problem is to output a set of $n$ linearly independent lattice vectors in an $n$-dimensional lattice, approximately of length$\\leq\\gamma\\lambda_n$.\nSIVP is NP-hard to approximate for any constant approximation factor.4 Shortest Basis Problem $\\left(\\mathrm{SBP}_\\gamma\\right)$ The $\\gamma$-approximate Shortest Basis Problem asks us to find a basis $B={\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}}$ for an $n$-dimensional lattice $\\mathcal{L}(B)$ such that $\\mathrm{max}_i \\lVert\\mathrm{b_i}\\rVert$ is the smallest possible upto a factor of $\\gamma$.\nLater in this post we will see the connection between the above problems.\nConstructing random instances of a Hard Lattice problem We now state some important results from Ajtai'961 and Ajtai'995 which deal with the construction of random instances of hard lattice problems. The following lemma shows that hardness of SBP by reduction to SVP.\nLemma 1 (Ajtai'96) If $\\mathrm{SBP}_\\gamma$ has no polynomial time solution for $\\gamma=\\mathrm{poly}(n)$, then we can generate a random lattice $\\Lambda$ (over some distribution $\\mathcal{D}$) together with a \u0026ldquo;short\u0026rdquo; vector $x\\in\\Lambda$ in polynomial time s.t. there is no algorithm which can find a vector shorter than $\\sqrt{n}$ in $\\Lambda$ over $\\mathcal{D}$ w.p. greater than $n^{-c}$, for sufficiently large $n$ and any $c\u0026gt;0$.\nNote: Here \u0026ldquo;short\u0026rsquo;\u0026rsquo; refers to $|x|\\leq\\sqrt{n}$. Later Ajtai'99 5 extended Lemma 1 to the following theorem:\nTheorem 2 (Main Theorem, Ajtai'99) If $\\mathrm{SBP}_\\gamma$ has no polynomial time solution for $\\gamma=\\mathrm{poly}(n)$, then we can generate a random lattice $\\Lambda$ (from the same distribution $\\mathcal{D}$ as in Lemma 1 together with a \u0026ldquo;short\u0026rdquo; basis $B$ of $\\Lambda$ in polynomial time s.t. there is no algorithm which can find a vector shorter than $\\sqrt{n}$ in $\\Lambda$ over $\\mathcal{D}$ w.p. greater than $n^{-c}$, for sufficiently large $n$ and any $c\u0026gt;0$.\nAs we see, Theorem 2 clearly gives us a way to construct a random instance of a hard problem ($\\mathrm{SBP}_\\gamma$). A short basis means $\\lVert \\mathrm{b_i} \\rVert\\leq n\\sqrt{n},i\\in[n]$, where $B=\\{\\mathrm{b_1},\\mathrm{b_2},\\ldots,\\mathrm{b_n}\\}$. In the next, we concretely define the class of \u0026ldquo;random\u0026rdquo; lattices required by the above results.\nThe SIS problem In this section, we take a deeper dive into the special class of random lattices given by Ajtai'961, where it was shown how to\nConstruct a family of one-way functions (based on the earlier family of random lattices as seen in Lemma 1), s.t., If we invert any function from this family of functions with non-negligible probability, it implies that we can solve any instance of the $\\mathrm{SVP}_\\gamma$ for $\\gamma=O({n^c}),c\u0026gt;1$. Therefore, by the previous discussion, we see that the above construction implies an average-case to worst-case reduction. GGH116 later extended the analysis of Ajtai96 to show that the family of one-way functions is also collision-resistant, which is a stronger and more useful property for cryptographic applications.\nWe now define the SIS problem and state a lemma (without proof) of its hardness.\nShort Integer Solution ($\\mathrm{SIS}_{n,m,q,\\beta}$) Let $A$ be a uniformly random matrix , i.e. , $A=\\{a_0| a_1 | \\ldots | a_{m-1}\\}$ consists of $m$ uniformly random vectors $a_i\\in\\mathbb{Z}^n_q,i\\in[m]$. Find a nontrivial $x\\in\\mathbb{Z}^m$, s.t. $|x|\\leq\\beta$ and $Ax\\equiv 0\\mod{q}$.\nHere $A\\sim\\mathbb{Z}_{q}^{n\\times m}$ where $\\mathbb{Z}^n_q$ is the set of all $n$-dimensional integer vectors modulo $q$, i.e., $\\mathbb{Z}^n_q={\\left\\langle x_1,\\ldots, x_n \\right\\rangle}$, where $x_i\\in{0,1,\\ldots,q-1}$ for all $i\\in[n]$.\nWe also note here that without the constraint $|x|\\leq\\beta$, the $\\mathrm{SIS}$ is easy to solve using Gaussian elimination.\nHardness of $\\mathrm{SIS}$ For any $m = \\mathrm{poly}(n)$, any $\\beta \u0026gt; 0$, and any sufficiently large $q \\geq \\beta n^c$ (for any constant $c \u0026gt;0$), solving $\\mathrm{SIS}({n,m,q,\\beta})$ with nonnegligible probability is at least as hard as solving $GapSVP_\\gamma$ and ${SIVP}_\\gamma$ for some $\\gamma = \\beta \\cdot O(\\sqrt{n})$ with a high probability in the worst-case scenario.\nNow we turn our attention to the construction of GGH116 based on the hardness of $\\mathrm{SIS}$.\nA specific CRHF construction (GGH11) We define a Collision-Resistant Hash function (CRHF) as follows:\nSet $m\u0026gt;n\\frac{\\log{q}}{\\log{d}}$. Key: A matrix $A$ chosen uniformly at random from $\\mathbb{Z}^{m\\times n}_q$. Hash function: Define a Shrinking function (shrinking since the domain is greater than the range): $f_A:{0,\\ldots, d-1}^m\\xrightarrow{}\\mathbb{Z}^n_q$ s.t. $f_A(x)=Ax\\mod q$. Goal: Find $x,x^{\\prime}\\in{0,1}^m$ s.t. $f_A(x)=f_A(x^{\\prime})$. Note that finding a solution to the collision problem yields a solution $z=x-x^\\prime$ to the $\\mathrm{SIS}$ problem as $0=f_A(x)-f_A(x^{\\prime})=Ax-Ax^\\prime=Az\\equiv 0\\mod{q}$.\nWe now define a class of random lattices known as $q$-ary lattices and connect it to the $\\mathrm{SIS}$ problem.\n$q$-ary Lattices at last! Given $A\\sim \\mathbb{Z}^{n\\times m}_q$, $q$-ary lattices $\\Lambda_q^\\perp(A)$ are defined as $$\\Lambda_q^\\perp(A)=\\{z\\in\\mathbb{Z}^m:Az=0\\mod{q}\\}.$$\nWe note that the randomness of $\\Lambda_q^\\perp(A)$ stems from the randomness of $A$ itself. $\\Lambda_q^\\perp(A)$ contains all vectors that are orthogonal modulo $q$ to the rows of $A$, i.e. $\\Lambda_q^\\perp(A)$ corresponds to the code whose parity check matrix is $A$.\nHere, we see that we have successfully characterized a lattice in terms of a parity check matrix $A$ instead of a basis. The class of $q$-ary lattices is the class of random lattices required in Lemma 1 and Theorem 2. We end this article with another lemma by Ajtai96 and some properties of $q$-ary lattices which connects the average-case hardness guarantees with worst-case guarantees.\nLemma 2 (Ajtai96) Finding short $z\\in\\Lambda_q^\\perp(A)$ s.t. $\\left|{z}\\right|\\leq \\beta \u0026lt; q$, implies solving $\\mathrm{GapSVP}_{\\beta\\sqrt{n}}$ on any n-dimensional lattice.\nProperties of $q$-ary Lattices A $q$-ary lattice $L$ satisfies $q\\mathbb{Z}^n \\subseteq L \\subseteq \\mathbb{Z}^n$ for $q\\in\\mathbb{Z}$. Any integer lattice $\\mathcal{L}\\subseteq\\mathbb{Z}^n$ is a $q$-ary lattice for some $q$, e.g., whenever q is an integer multiple of the determinant $\\mathrm{det}(\\mathcal{L})$. We are only interested when $q$ is much smaller than $\\left|{\\mathrm{det}(L)}\\right|$. The following two problems are equivalent: Finding a short vector in $\\Lambda_q^\\perp(A)$ s.t. $A\\sim \\mathbb{Z}^{n\\times m}_q$. Finding a short solution to a set of $n$ random equations modulo $q$ in $m$ variables. The dual of any $q$-ary lattice is also a $q$-ary lattice. We represent the dual of a $q$-ary lattice $\\Lambda_q^\\perp(A)$ using the notation $\\Lambda_q(A)$, which is defined as $$\\Lambda_q(A)=\\{z\\in\\mathbb{Z}^m:z=A^Ts\\mod{q},,s\\in\\mathbb{Z}^n\\}.$$ Here $\\Lambda_q(A)$ is the code generated by the rows of $A$, i.e., $A$ itself is the generator matrix of the code. Contrast this with $\\Lambda_q^\\perp(A)$ where $A$ is the parity check matrix. References Ajtai96: M. Ajtai. 1996. Generating hard instances of lattice problems (extended abstract). In Proceedings of the twenty-eighth annual ACM symposium on Theory of Computing (STOC \u0026lsquo;96). Association for Computing Machinery, New York, NY, USA, 99–108. https://doi.org/10.1145/237814.237838\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShor95: Peter W. Shor. 1997. Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer. SIAM J. Comput. 26, 5 (Oct. 1997), 1484–1509. https://doi.org/10.1137/S0097539795293172\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBP23: Huck Bennett and Chris Peikert. Hardness of the (Approximate) Shortest Vector Problem: A Simple Proof via Reed-Solomon Codes. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2023). Leibniz International Proceedings in Informatics (LIPIcs), Volume 275, pp. 37:1-37:20, Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2023). https://doi.org/10.4230/LIPIcs.APPROX/RANDOM.2023.37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBS99: Johannes Blömer and Jean-Pierre Seifert. On the complexity of computing short linearly independent vectors and short bases in a lattice. In Proceedings of the Thirty-first Annual ACM Symposium on Theory of Computing, STOC ’99, pages 711–720, New York, NY, USA, 1999. ACM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAjtai99: Miklós Ajtai. 1999. Generating Hard Instances of the Short Basis Problem. In Proceedings of the 26th International Colloquium on Automata, Languages and Programming (ICAL \u0026lsquo;99). Springer-Verlag, Berlin, Heidelberg, 1–9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGGH11: O. Goldreich, S. Goldwasser, and S. Halevi. Collision-free hashing from lattice problems. Studies in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and Computation: In Collaboration with Lidor Avigad, Mihir Bellare, Zvika Brakerski, Shafi Goldwasser, Shai Halevi, Tali Kaufman, Leonid Levin, Noam Nisan, Dana Ron, Madhu Sudan, Luca Trevisan, Salil Vadhan, Avi Wigderson, David Zuckerman, pages 30–39, 2011\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://theoreticles.netlify.app/posts/qary/","summary":"\u003cp\u003eIn this post, we discuss an important class of algebraic structures known as $q$-ary lattices that are central to lattice-based cryptographic primitives.\u003c/p\u003e\n\u003ch2 id=\"hardness-of-problems\"\u003eHardness of problems\u003c/h2\u003e\n\u003cp\u003eComputational hardness usually revolves around problems with \u003cstrong\u003eworst-case hardness\u003c/strong\u003e guarantees since we want to design algorithms that run efficiently even on the worst possible input.\u003c/p\u003e\n\u003cp\u003eOn the other hand, cryptographic schemes require security guarantees for random keys. Therefore, cryptographic applications require problems with \u003cstrong\u003eaverage-case hardness\u003c/strong\u003e guarantees.\u003c/p\u003e","title":"$q$-ary Lattices"},{"content":" This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. Don\u0026rsquo;t hesitate to contact me via email if you need further clarification. Without further ado, let\u0026rsquo;s dive in.\nIntroduction The What and Why of reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs. Therefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\nMost unsolved problems we encounter are already related to some existing problem we know about.\nThis relation to solved problems is something that we can now prod and prick incessantly and eventually use to arrive at a conclusion about new problems. While hardly elegant in its conception, this approach is highly sophisticated in its execution. This is the basis for the mathematical technique known as reduction.\nWhen can we use reduction techniques? First we start by describing two types of problems: hard and easy! Hard problems are typically those that the problem solver does not know how to solve or knows are very hard to solve, while easy problems are simply easy to solve.\nSince the notion of hardness as described above is quite subjective and non-rigorous, we formalize it below by quantifying the capabilities of the problem solver.\nThroughout this post, we assume that the notion of hard and easy depends only on the computational resources available to the problem solver.\nAccess to powerful computational models might allow us (the problem solver) to efficiently find solutions to previously hard-to-solve problems. For example, SAT can be easily solved if we have know how to solve the Halting problem. We shall expand on this example later in this post.\nTake two problems, $O$ (an old problem) and $N$ (a new problem). Suppose we make the astute observation that the underlying mathematical structure of $N$ is similar to $O$. Therefore, instead of trying to solve $N$ from scratch, we can hope to somehow use $O$ to solve $N$. Consider the following situations of interest that might arise where we would ideally like to avoid reinventing the wheel.\nCase 1: $O$ is harder to solve than $N$, but $O$ is actually easy to solve In this case, mathematically, we say that $N\\subseteq O$^[Here, we consider $O$ and $N$ to be languages or sets of strings c orresponding to decision problems, i.e., YES/NO problems.]. In this case, our primary approach is to transform instances of $N$ into instances of $O$ and then use the algorithm for $O$ to solve for $N$. In other words, if we can demonstrate that $N$ is a special case of $O$, we solve $N$ using our existing knowledge of $O$.\nReducing problem $A$ to $B$ is the process of transforming instances of $A$ into instances of $B$. Note that this is only possible if $A\\subseteq B$.\nCase 2: $O$ is easier to solve than $N$, but $O$ is actually hard to solve In this case, we can again exploit the structural similarities between $O$ and $N$, but this time to a different end. Without prior context, it is difficult to show that $N$ is hard to solve. However, using the hardness of $O$, it is easy to show that $N$ is hard to solve as well.\nWe start with the assumption that $N$ is easy to solve, i.e., there exists some algorithm to efficiently decide membership in the set $N$. We use this assumption to arrive at a contradiction. Now exploiting the similarities between $O$ and $N$, we transform all instances of $O$ to some instance of $N$.\nSince $O$ reduces to $N$, i.e., there is a way to efficiently decide membership in $O$ using the algorithm for $N$. However, we know for sure that $O$ is hard to solve. This leads us to a contradiction, which is resolved by removing our assumption that $N$ is easy to solve. Therefore $N$ has to be hard to solve as well.\nLet us explore a few properties of reductions now.\nRelative Hardness For any pair of languages / decision problems $A$ and $B$, if $A\\subseteq B$, then $B$ is at least as hard as $A$ to solve, i.e., $A$ cannot be harder to solve than $B$.\nReductions as relations Reflexivity of reductions: Trivially $A\\subseteq A$ by using identity transformations. Transitivity of reductions: If $A\\subseteq B$ and $B\\subseteq C$ then we can prove that $A\\subseteq C$ by composing the transformations. Reductions are, however, not symmetric relations (and, therefore, by extension, not equivalence relations). $A\\subseteq B$ does not imply $B\\subseteq A$. A concrete example of this will be discussed later in detail: We can reduce SAT, which is a decidable problem, to the Halting problem, which is an undecidable problem, but the converse is not true (by definition of decidability).\nAn alternate view of reductions Reductions are a highly general family of techniques, and to provide a rigorous formalization of reductions, we consider some specific variants. An interesting way of looking at reductions is as follows:\nGiven an oracle for a problem $B$, can we solve $A$ efficiently by making oracle calls to $B$? If yes, then $A\\subseteq B$.\nHere, an oracle is nothing but a black-box subroutine for efficiently solving a problem. The beauty of reductions lie in the fact that we do not need to bother about the internal mechanisms of the oracle itself. Nor do we have to worry about construct the oracle itself. We simply have to use the oracle\u0026rsquo;s existence. Implicitly, reduction is a two-step process. Suppose $A\\subseteq B$.\nSuppose there exists a subroutine $R$ that transforms yes instances of $A$ into yes instances of $B$, and no instances of $A$ into no instances of $B$. First, input instances $x$ (that may/may not belong to $A$) are reduced/transformed into possible instances of $B$ using the subroutine $R$. Next, we perform invocation(s) to the oracle for $B$ to decide the membership of $R(x)$ in $B$. Explicitly, we perform the following computation: $O_B(R(x))$. If $O_B(R(x))=1$, then we decide that $x\\in A$. Otherwise, we decide that $x\\notin A$. $$x\\in A \\iff R(x) \\in B.$$ The notion of efficiency is twofold.\nFirstly, we are concerned with how efficient the transformation/reduction from $x$ to $R(x)$ is. The notion of efficiency in transforming $x$ to $R(x)$ has a small caveat, which we explore via an example. Only the construction of the reduction method $R$ needs to be efficient, and solving $O_B(R(x))$ does not need to be efficient. Consider the following C pseudocode: int A(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } Writing this piece of code took finite time, and was certainly efficient. However, depending on the value of the input the for loop will never terminate for values of $x\u0026gt;11$. Therefore, this program may become inefficient during runtime. The notion of efficiency we consider during reductions (unlike computational scenarios) is how efficiently we can write the code for function A, and not how efficiently A transforms $x$ into $R(x)$. Sometimes, we might require more than one call to the oracle $O_B$, depending on the problems at hand. In this case, we are concerned with how many times the oracle is invoked. A Prototypical Reduction: SAT to the Halting Problem Note: This is a highly technical section, and would need familiarity to the satisfiability problem and the Halting problem. We encourage the reader to familiarize themselves with the formal definitions of these problems first before going through this section. We give an intuitive description of the problems below.\nThe Halting Problem Recall the earlier piece of pseudocode with a slight modification.\nint A1(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } int A2(int x){ for(;x\u0026gt;12;x--);// This loop will always terminate return x; // This is R(x) } Note A2 will always halt no matter the input, while A1 may never halt depending on the input.\nImagine you would like to design a function $B$ that takes the binary description of any single parameter function $A$ and any arbitrary input $x$, and find out whether $A$ will halt on input $x$.\nThe Halting problem (HALT): Given (the binary encoding of) any arbitrary Turing Machine $A$ and an arbitrary input $x$ to $A$, does $A(x)$ halt?\nThis in effect describes the Halting problem, where $B$ is a universal Turing machine and $A$ can be any Turing Machine. It has been shown that there does not exist any such $B$ which solves this problem. Therefore the Halting problem cannot be decided, and is formally referred to as an undecidable problem.\nThe SAT Problem A Boolean formula $f$ accepts as input an $n$-bit string and outputs a $1$ (if it accepts the string), or a $0$ (if it rejects the string). Mathematically, $f:{0,1}^{n}\\xrightarrow{}{0,1}$.\nThe satisfiability(SAT) problem: Given a Boolean formula $f$ and an arbitrary $n$ bit string $x\\in{0,1}^{n}$, is $f(x)=1$?\nNote that the structure of SAT is subsumed by the structure of the Halting problem. If the decider to Halting problem ever halts, we can find out whether $f(x)=1$.\nEven though SAT can be quite hard to solve computationally (may have exponential runtime depending on the structure of the formula), we can always construct an algorithm to solve it. Therefore, right off the bat, we observe that SAT is easier to solve (it is decidable) than the Halting problem.\nReducing SAT to HALT Let us finally have a look at how we would reduce SAT to the Halting problem.\nOur input $x$ is a Boolean formula. We want to output if this formula is satisfiable.\nWe construct a TM (Turing Machine) $T$ which accepts $x$ and does the following:\n$T$ iterates over all possible assignments to find a satisfying assignment. 🔴 This may require exponential runtime in the size of the formula. If $T$ finds a satisfying assignment, halt and return 1. 🟣 Hence, if x is satisfiable, T halts. Otherwise, we put $T$ into an infinite loop. 🟣 Hence, T halts iff if x is satisfiable. Our reduction $R(x)=\\langle\\langle T\\rangle,x\\rangle$ takes the SAT formula $x$ and returns an encoding of the above Turing machine $T$ coupled with $x$, s.t., yes instances of SAT map to yes instances of HALT, and no instances of SAT map to no instances of HALT. 🟣 Note that $R(x)$ at this point can be compared to a compiled binary which has not yet been executed.\nWe pass $R(x)$ to $O_{\\text{HALT}}$.\nIf $O_{\\text{HALT}}(R(x))$ returns yes, this implies $T$ halts on input $x$, which in turn implies $x$ has a satisfying assignment. Therefore $R(x)\\in\\text{HALT}\\implies x\\in\\text{SAT}$. If $O_{\\text{HALT}}(R(x))$ returns no, then $T$ does not halt on input $x$, which implies that $x$ does not have a satisfying assignment. Therefore $R(x)\\notin\\text{HALT}\\implies x\\notin\\text{SAT}$. We can take the contrapositive of this expression to obtain $x\\in\\text{SAT}\\implies R(x)\\in\\text{HALT}$. Once again we note the following.\nIn step 2, we are simply constructing the TM $T$, not executing it.\nThe reduction is the transformation of the SAT formula $x$ to an encoding of both the Turing Machine and the SAT formula, which is an instance of the Halting problem (HALT$_{TM}$ requires an arbitrary TM and an input string to the TM).\nTaxonomy of Polynomial-time reductions In this post, we only consider polynomial-time reductions, i.e.,\nthe time taken to transform $x$ to $R(x)$, and the number of calls to $O_B$ are both polynomial. These are the most commonly studied types of reductions, and we look at three kinds of polynomial-time reductions.\nKarp Reductions / Many-one reductions These are the most restrictive type of polynomial reductions. Given a single input $x$, $R(x)$ produces a single instance $y$ such that $x\\in A\\iff y\\in B$. Therefore, we have to perform only one oracle call to $O_B$. The earlier reduction from SAT to HALT was a Karp reduction.\nWe can choose to strengthen the notion of Karp reductions to include weaker forms of reductions. For example, in logspace many-one reductions, we can compute $R(x)$ using just logarithmic space instead of polynomial time. Even more restrictive notions of reductions consider reductions computable by constant depth circuit classes.\nTruth Table Reductions These are reductions in which given a single input $x$, $R(x)$ produces a constant number of outputs $y_1,y_2,\\ldots, y_k$ to $B$. The output $O_A(x)$ can be expressed in terms of a function $f$ that combines the outputs $O_B(y_i)$ for $i\\in[1,\\ldots,k]$.\nLet us assume that $f$ outputs $1$ for the desired combination and $0$ otherwise. $$ x\\in A\\iff f\\left(y_1\\in B, y_1\\in B,\\ldots,y_k\\in B\\right)=1 $$ Here, $f$ is efficiently computable, and there are a constant ($k$) number of oracle calls to $O_B$.\nLet us consider an example. Consider two problems on a graph $G$ with a constant number of vertices $\\ell$.\n$A$: What is the minimum sized independent set for $G$? $B$: Does $G$ have an independent set of size $k$?\nThe reduction $A\\subseteq B$ would involve looping from $1$ to $\\ell$ and querying $B$ each time. The combination function would be an OR function. At the first yes instance of $B$, we return the value as the answer for $A$. If there is no independent set in $G$, the worst number of calls to $O_B$ is $\\ell$.\nCook Reductions / Poly-time Turing Reductions Here, we are allowed a polynomial number of oracle calls and polynomial time for transforming the inputs. These are the most general form of reductions, and the other forms of reductions are restrictions of Cook reductions. In the example graph $G$ used for TT reductions, if we assume the number of vertices of $G$ to be a polynomial, then the reduction $A\\subseteq B$ using the same exact process would be a Cook reduction.\n$A\\subseteq_{m} B \\implies A\\subseteq_{t} B \\implies A\\subseteq_{T} B$ where, $\\subseteq_{m}$ denotes Karp reductions, $\\subseteq_{t}$ denotes Truth Table reductions, and $\\subseteq_{T}$ denotes Cook reductions.\nAside: From the nature of the examples, we can see that Karp reductions only extend to decision problems (problems with yes/no outputs). In contrast, Cook reductions can accommodate search/relation/optimization problems (problems with a set of outputs).\nBasic reductions in Computability Theory In this section, the reader is assumed to have familiarity with concepts of decidability and undecidability. Let us now proceed with some instances of reductions in computability theory.\nLet $A\\subseteq B$, and they are decision problems.\nIf $A$ is decidable, what can we say about $B$? If $A$ is semi-decidable, what can we say about $B$? If $A$ is undecidable, what can we say about $B$? We know that $B$ is at least as hard as $A$ since $A\\subseteq B$. Therefore in the first case, $B$ may be decidable, semi-decidable, or undecidable. In the second case, $B$ may be semi-decidable, or undecidable. In the third case, $B$ is undecidable.\nThe third case is of interest here - To show that a $B$ is undecidable, we must find a reduction from $A$ to $B$, where $A$ is already known to be undecidable. Note (Advanced): The reduction function must itself be a computable function.\nNow again, consider $A\\subseteq B$ where they are decision problems. We can conclude the following:\nIf $B$ is decidable, $A$ is decidable. If $B$ is semi-decidable, $A$ is semi-decidable. If $A$ is not decidable, then $B$ is not decidable. If $A$ is not semi-decidable, then $B$ is not semi-decidable. $A\\subseteq B$, then $\\bar{A}\\subseteq \\bar{B}$, where $A$ and $B$ are decidable problems. The first two statements are true, as discussed above, while 3 is the contrapositive of the first statement and the fourth statement is the contrapositive of the second statement. For the fifth statement, the proof is as follows:\n$x\\in \\bar{A} \\iff x\\notin A \\iff R(x)\\notin B \\iff R(x)\\in\\bar{B}$. Therefore, $x\\in \\bar{A} \\iff R(x)\\in\\bar{B}$.\nImmediately we see the power of formalizing the notion of reductions.\nComplexity-Theoretic notions This section is for advanced readers only due to the number of prerequisites involved.\nA complexity class is a set of computational problems that can be solved using similar amounts of bounded resources (time, space, circuit depth, number of gates, etc.) on a given computational model (Turing machines, circuits, cellular automaton, etc.).\nThe complexity classes $\\mathrm{P}$, $\\mathrm{NP}$, and $\\mathrm{PSPACE}$ are closed under Karp and logspace reductions. The complexity classes $\\mathrm{L}$ and $\\mathrm{NL}$ are closed only under logspace reductions. Closure has the following semantics - given a decision problem $A$ in a complexity class $C$, any problem $B$ such that $B\\subseteq A$ is also in $C$.\nWe now explore two interesting notions in complexity theory that arise from reductions.\nCompleteness For a bounded complexity class,\nComplete problems are the hardest problems inside their respective complexity class.\nA more formal definition of completeness is as follows:\nGiven a complexity class $C$ which is closed under reduction $r$, if there exists a problem $A$ in $C$ such that all other problems in $C$ are $r$-reducible to $A$, $A$ is said to be C-complete.\nFor example, an NP-complete problem is in NP, and all problems in NP are Karp-reducible to it. The notions of PSPACE-completeness and EXPTIME-completeness are similarly defined under Karp-reductions.\nThe role of reductions in this context can be understood through P-completeness. Consider any non-trivial decision problem in P (trivial problems are akin to constant functions). Every other non-trivial decision problem is Karp-reducible to it. Therefore every non-trivial decision problem is P-complete under Karp-reductions. This definition is essentially the same as P (minus the empty language and $\\Sigma^*$). Therefore, we come to the following conclusion:\nUnder Karp-reductions, the notion of P-completeness is semantically useless.\nTherefore, we use weaker forms of reductions such as logspace reductions or reductions using constant depth circuit computable functions to achieve a more meaningful notion of P-completeness.\nSelf reducibility Decision problems are yes/no problems where we ask if a solution exists. However, sometimes we also want to solve the corresponding search problem to find a solution (if one exists).\nIn the context of languages in NP, self-reducibility essentially states that if we can efficiently solve the decision version of a problem, we can also efficiently solve the search/optimization version of the problem. A more formal definition would be as follows - the search version of a problem Cook-reduces (polynomial-time Turing-reduces) to the decision version of the problem. Every NP-complete problem is self-reducible and downward self-reducible.\nConclusion This write-up aims to demystify a core technique used in theoretical computer science and provide a few contexts for its usage. For a more rigorous and formal introduction to this topic, please check out Michael Sipser\u0026rsquo;s excellent book \u0026ldquo;An Introduction to the Theory of Computation.\u0026rdquo; There are many more things to say about reductions, and this article barely scratches the surface on any of the topics it touches. However, I feel it would be prudent to stop at this point before it becomes any more of a rambling mess than it already is.\n","permalink":"https://theoreticles.netlify.app/posts/reductions/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eThis post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. Don\u0026rsquo;t hesitate to contact me via email if you need further clarification. Without further ado, let\u0026rsquo;s dive in.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"the-what-and-why-of-reductions\"\u003eThe What and Why of reductions?\u003c/h3\u003e\n\u003cp\u003eFrom Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.\nTherefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\u003c/p\u003e","title":"An Introduction to Reductions"}]