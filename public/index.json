[{"content":"QTML 2022 Once thought to be a niche curiosity, the field of QML is growing astoundingly fast. QTML, or \u0026lsquo;\u0026lsquo;Quantum Techniques in Machine Learning\u0026rsquo;\u0026rsquo; is an annual conference that caters to its eponymous field. The conference describes itself as \u0026lsquo;focused on the interplay between machine learning and quantum physics.\u0026rsquo; For an overview of various subfields of QML, check out this survey article by Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd from 2017. If you are more interested in the complexity-theoretic and learning-theoretic aspects, this guest article by Arunachalam and Ronald de Wolf serves as a good introduction. Finally, I would like to list Peter Wittek\u0026rsquo;s magnum opus as a keystone in the foundation of the entire QML field.\n‚ÄúRome is stately and impressive; Florence is all beauty and enchantment; Genoa is picturesque; Venice is a dream city; but Naples is simply ‚Äî fascinating.‚Äù ~ Lilian Whiting\nThis year was the sixth iteration of the conference, having been hosted earlier in Verona, Italy (2017), Durban, South Africa (2018), and Daejeon, South Korea (2019), with a couple of online fixtures due to the pandemic. The location for this year was at the Centro Congressi University of Naples Federico II in Naples, Italy, which is situated right on the coast across from the majestic Castel dell Ovo (Castle of the Egg). The conference lasted six days (7th-12th of November), with pre-conference and post-conference events and workshops.\nOur paper on \u0026ldquo;Quantum Boosting using Domain-Partitioning Hypotheses\u0026rdquo; (see preprint here) was selected for a short talk. In this work, we extend the QAdaBoost paper (which deals with weak learners producing only binary hypotheses) to a more general framework that allows us to adaptively boost weak learners that output the aforementioned \u0026ldquo;Domain-Partitioning Hypotheses\u0026rdquo;. A natural example of these learners would be decision trees or any sort of clustering algorithms.\nMe during the talk A poster for our work Before we go on, enjoy a few pics in and around the conference venue!\nView from the breakroom 1 View from the breakroom 2 Me in front of Castel dell\u0026rsquo;Ovo The Conference Hall The conference is happening at CERN next year!\nThere were about a 100 attendees with participants mainly from Europe, some small groups from the US, South Africa, South Korea, and one Indian (me!). There was a good mix of industry (companies like Zapata, Xanadu, QuNova had sent in talks and representatives) and academia (with faculty, postdocs, PhDs, and a couple of undergrads). I was very excited to connect with a lot of QML folks, most of whom I had only stalked on arXiv thus far.\nQTML in Tweets In this section I go over a few talks in the form of tweets (mainly because that is what the cool academics do in 2022!).\nThe Peter Wittek Session The Peter Wittek session consisted of a number of talks by distinguished speakers all of whom had personal anecdotes to share about Peter and their views on how he contributed to the nascent field of QML.\nJacob Biamonte\u0026rsquo;s tribute (1/5) I was invited to give a small tribute today to one of the early researchers that entered quantum computing from a machine learning perspective. He is missed. pic.twitter.com/tH7K1wGXwg\n‚Äî Jacob Biamonte (@JacobBiamonte) November 8, 2022\nSeth Lloyd on QML The Peter Wittek session is underway at #qtml 2022, after Seth Lloyd\u0026rsquo;s overview of quantum machine learning. pic.twitter.com/SRK65pmi1t\n‚Äî Sagnik Chatterjee (@chatsagnik) November 8, 2022\nGitta Kutyniok\u0026rsquo;s keynote talk A fascinating theoretical perspective into hardness results on explainability and reliability by @GittaKutyniok at QTML2022 #qtml2022. pic.twitter.com/7oTkpo6Odf\n‚Äî Sagnik Chatterjee (@chatsagnik) November 8, 2022\nJacob Biamonte\u0026rsquo;s keynote talk on VQA Today\u0026rsquo;s illuminating keynote speech by @JacobBiamonte at QTML on a host of topics related to variational quantum algorithms. pic.twitter.com/j6MAEXnI3l\n‚Äî Sagnik Chatterjee (@chatsagnik) November 9, 2022\n#QTML slides: \u0026ldquo;On parameterisation effects in training variational quantum circuits\u0026rdquo;\nThanks to @Alessdip and other organisers. https://t.co/LNio4y6LT7\n‚Äî Jacob Biamonte (@JacobBiamonte) November 10, 2022\nQuantum State Preparation by Fereshte Mozafari Fereshte Mozafari talks about an alternative way for Quantum State Preparation at #QTML 2022 today. pic.twitter.com/Ru5MsrXpXd\n‚Äî Sagnik Chatterjee (@chatsagnik) November 10, 2022\nQuantum Recurrent Neural Networks by Michal Siemaszko Michal Siemaszko from @UniWarszawski Warsaw (from @StobinskaQCAT group) presenting today at #QTML 2022 in Naples our work on quantum recurrent networks in collab w/ @ESA_EO #Philab (visiting researcher spring 2022).\nArxiv: https://t.co/QIjTvPUFxE#QC4EO pic.twitter.com/LrOUzrfSxN\n‚Äî Bertrand Le Saux (@blesa_ux) November 9, 2022\nAvoiding barren plateaus in VQAs by Antonio A. Mele I am thrilled to have presented my talk today at #QTML 2022. It was my first scientific talk in front of so many people.\nYou can find the slides on my website here: https://t.co/EPMNKlWtxH\n‚Äî Antonio Anna Mele (@QuAntonioMele) November 9, 2022\nTalks by Quantinuum My colleagues @LuukCoopmans and @Sam_Duffield will present their research at #QTML in Naples this week.\n1Ô∏è‚É£ Thu: Predicting Gibbs State Expectation Values with Pure Thermal Shadows\n2Ô∏è‚É£ Fri: Bayesian Learning of Parameterised Quantum Circuits\nIf you\u0026rsquo;re there, consider attending\n‚Äî Matthias Rosenkranz ‚ú® (@rosenkranz) November 9, 2022\nFinal Day Today\u0026rsquo;s QTML personal highlights so far.\n1. @QuantumManuel\u0026rsquo;s talk on how classical Tensor networks can be useful to simulate shallow quantum circuits.\n2. @MartinLaroo on group-invariant QML motivated by geometric DL.\nI have a talk later in the day so feel free to catch up! pic.twitter.com/Dgut1Vmxqk\n‚Äî Sagnik Chatterjee (@chatsagnik) November 11, 2022\nThe Quantum Software Workshop At today\u0026rsquo;s Workshop on #QuantumSoftware (co-located with #QTML), @QuantumMato presented our tools for #QuantumComputing.üëçüôÇ\nüëâOverview: https://t.co/kxZPcaO8V1\nüëâCode/github: https://t.co/uO7unDWbQh pic.twitter.com/KSIZTLPA4C\n‚Äî Robert Wille (@rbrtwll) November 12, 2022\nNon-technical stuff If you have stuck around thus far, the rest of the blog contains stuff about my travel and experiences in Italy.\nVISA struggles The plight of COVID Ph.D.\u0026rsquo;s in terms of international travel is indescribable. More than just getting your paper accepted at a conference is required. You have to wait for all the threads of fate to align to be able to actually present the paper.\nItaly has a (very weird) residential constraint for VISA applications. You can only be serviced by a VFS center that falls within your residential jurisdiction (i.e., your place of residence for the last six months). This makes the already daunting challenge of finding open slots at VFS centers nearly impossible to overcome. I got a slot for 21st October (purely by luck) at the Delhi center after the usual shenanigans of clicking refresh multiple times a day. The visa was processed quite fast - it took me less than five working days to receive the visa.\nNext came the challenge of procuring Euro bills for the journey since a few seniors suggested that Italy was less digitized than India. What should have been a straightforward endeavor became an adventure in the dusty narrow corridors of the dilapidated buildings of Nehru Place. I set out to find agencies that could arrange cash on short notice (by the time I received the visa, I had less than a week till my flight to Rome.) The entire experience was highly shady, but surprisingly everything turned out alright.\nGetting there I left for Rome on the night of 6th. Due to extremely short transit times (around 1.5 hrs) at Doha, I did not have the time to explore the airport much, but I managed to snag a pic with these two beauties.\nAfter landing in Rome at the Fiumicino airport, I took a train to the Roma Terminus, and then another one to Napoli Centrale. The views of the Italian countryside with its gentle undulating hills and idyllic pastures seemed straight out of Tolkein\u0026rsquo;s description of middle earth.\nA word of advice for anyone travelling to Italy. You will find appropriate (daily/weekly) passes for travelling on the public transport at the local Tabachchi\u0026rsquo;s (cigarette shops). I recommend getting these, since public transit (buses, trams, metro) is good enough in both Rome and Naples to cover almost of all the local hotspots.\nSightseeing and Food! Even though Naples is primarily known for its eponymous styled pizza, Naples has a lot more to offer than just pizza! I could not carry out too detailed of an investigative tour of Rome\u0026rsquo;s culinary scene this time, but I think the Trevi fountain cent has me covered for a return trip.\nI also visited a few churches and castles in and around the city center during the evenings after the day\u0026rsquo;s events ended. After the conference, I had a few days in hand before my return flight, which allowed me to visit the ruins of Pompeii (I booked a trip on TripAdvisor along with a group of fellow conference attendees) and carry out a whirlwind checklist tour in Rome. Enjoy the pics!\nClockwise from top left: Caprese Salad, Cheese Sandwich with Chocolate Croissant and a double shot of espresso, Fish and Chips, Arancino, Seafood Risotto Neopolitan Pizza Margherita Various shots of Pompeii and Rome Clockwise from top left: A theatre in Pompeii (first 2 pics), The Colosseum (next two), Trevi Fountain, The Plaza at Pompeii, Castel Sant\u0026rsquo;Angelo, Vatican City, Inside St. Peter\u0026rsquo;s Basilica. As they say\n‚ÄúVedi Napoli e poi muori! ‚Äî See Naples and die!‚Äù\nI am completely enamored with the Amalfi coast\u0026rsquo;s beauty and counting the days until I return!\nI will keep adding stuff to this post, as and when I find time.\n","permalink":"https://theoreticles.netlify.app/posts/qtml/","summary":"QTML 2022 Once thought to be a niche curiosity, the field of QML is growing astoundingly fast. QTML, or \u0026lsquo;\u0026lsquo;Quantum Techniques in Machine Learning\u0026rsquo;\u0026rsquo; is an annual conference that caters to its eponymous field. The conference describes itself as \u0026lsquo;focused on the interplay between machine learning and quantum physics.\u0026rsquo; For an overview of various subfields of QML, check out this survey article by Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd from 2017.","title":"QTML 2022"},{"content":"This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. Don\u0026rsquo;t hesitate to contact me via email if you need further clarification. Without further ado, let\u0026rsquo;s dive in.\nIntroduction Why do we need reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.\nTherefore, we must develop principled approaches and techniques to solve new and unseen problems. Here is the good part:\nMost unsolved problems we encounter are already related to some existing problem we know about.\nThis relation to solved problems is something that we can now prod and prick incessantly and eventually use to arrive at a conclusion about new problems. While hardly elegant in its conception, this approach is highly sophisticated in its execution. This is the basis for the mathematical technique known as reduction.\nWhen can we use reduction techniques? First we start by describing two types of problems: hard and easy! Hard problems are typically those that the problem solver does not know how to solve or knows are very hard to solve, while easy problems are simply easy to solve.\nSince the notion of hardness as described above is quite subjective and non-rigorous, we formalize it below by quantifying the capabilities of the problem solver.\nThroughout this post, we assume that the notion of hard and easy depends only on the computational resources available to the problem solver.\nAccess to powerful computational models might allow us (the problem solver) to efficiently find solutions to previously hard-to-solve problems. For example, SAT can be easily solved if we have know how to solve the Halting problem. We shall expand on this example later in this post.\nTake two problems, $O$ (an old problem) and $N$ (a new problem). Consider the two following situations that might arise where we would ideally like to avoid reinventing the wheel.\nWe know that $O$ is easy to solve, and we notice that the underlying mathematical structure of $N$ is similar to $O$. We know that $O$ is hard to solve, and once again, $N$ is similar to $O$. Case 1: $O$ is easy to solve In the first case, our primary approach is to transform instances of $N$ into instances of $O$ and then use the algorithm for $O$ to solve for $N$. In other words, using our existing knowledge of $O$, we solve $N$ and demonstrate that $N$ is a special case of $O$.\nMathematically, we say that $N\\subseteq O$ or $N$ reduces to $O$.\nCase 2: $O$ is hard to solve In the second case, we can again exploit the structural similarities, but this time to a different end. Without prior context, it is difficult to show that $N$ is hard to solve. Therefore, we start with the assumption that $N$ is easy to solve. Now exploiting the similarities between $O$ and $N$, we transform all instances of $O$ to some instance of $N$. This gives rise to a contradiction. We have apparently shown that $O$ reduces to $N$, but we can prove that $O$ is hard to solve. Therefore $N$ has to be hard to solve as well. Otherwise, we could use the algorithm for $N$ to solve for $O$.\nLet us explore a few properties of reductions now.\nRemarks on reductions Relative Hardness If $A\\subseteq B$, then $B$ is at least as hard as $A$ to solve, i.e., $A$ cannot be harder to solve than $B$.\nReductions as relations Trivially $A\\subseteq A$ by using identity transformations. Therefore reductions are reflexive relations. If $A\\subseteq B$ and $B\\subseteq C$ then we can prove that $A\\subseteq C$ by composing the transformations. Therefore reductions are transitive relations. Reductions are, however, not symmetric relations (and, therefore, by extension, not equivalence relations). As stated earlier, if $A\\subseteq B$, then $B$ is at least as hard or harder to solve than $A$. Therefore,\n$A\\subseteq B$ does not imply $B\\subseteq A$. A concrete example of this will be discussed later in detail: We can reduce SAT, which is a decidable problem, to the Halting problem, which is an undecidable problem, but the converse is not true (by definition of decidability).\nFormalizing reductions The observant reader will notice that until this point, we have not provided an actual mathematical definition of reductions. The reason for this is simple. Reductions are a highly general family of techniques, and to provide a rigorous formalization of reductions, we consider some specific variants.\nA formal way of looking at reductions is as follows:\nGiven an oracle for a problem $B$, can we solve $A$ efficiently by making oracle calls to $B$? If yes, then $A\\subseteq B$.\nHere, an oracle is nothing but a black-box subroutine for efficiently solving a problem. We do not need to bother about the internal mechanisms of the oracle itself - we just have to use the oracle\u0026rsquo;s existence.\nImplicitly this is a two-step process.\nFirst, instances of $A$ are reduced/transformed into instances of $B$. Therefore $x\\in A$ is transformed into $y\\in B$. Since $x$ is \u0026ldquo;reduced/transformed\u0026rdquo; into $y$, we shall use $R(x)$ to denote instances of $B$. Now we perform invocation(s) to the oracle for $B$ to solve for these transformed instances $R(x)$ or $y$. Depending on the output of the $B$ oracle, we decide our output for the $A$ oracle. We denote this mathematically as $x\\in A \\iff R(x) \\in B$.\nThe notion of efficiency is twofold.\nFirstly, we look at how efficient the transformation/reduction from $x$ to $R(x)$ is. Secondly, for the oracle $O_B$, we are concerned with how many times the oracle is invoked. The notion of efficiency in transforming $x$ to $R(x)$ has a small caveat, which we explore via an example.\nOnly the construction of the reduction method $R$ needs to be efficient, and solving $O_B(R(x))$ does not need to be efficient.\nFor example consider the following C pseudocode:\nint A(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } Writing this piece of code took finite time, and was certainly efficient. However, depending on the value of the input the for loop will never terminate for values of $x\u0026gt;11$. Therefore, this program may become inefficient during runtime. The notion of efficiency we consider during reductions (unlike computational scenarios) is how efficiently we can write the code for function A, and not how efficiently A transforms $x$ into $R(x)$.\nReducing SAT to the Halting Problem Note: This is a highly technical section, and would need familiarity to the satisfiability problem and the Halting problem. We encourage the reader to familiarize themselves with the formal definitions of these problems first before going through this section. We give an intuitive description of the problems below.\nThe Halting Problem Recall the earlier piece of pseudocode with a slight modification.\nint A1(int x){ for(;x\u0026gt;12;x++); return x; // This is R(x) } int A2(int x){ for(;x\u0026gt;12;x--);// This loop will always terminate return x; // This is R(x) } Note A2 will always halt no matter the input, while A1 may never halt depending on the input.\nImagine you would like to design a function $B$ that takes the binary description of any single parameter function $A$ and any arbitrary input $x$, and find out whether $A$ will halt on input $x$.\nThe Halting problem: Given a Turing Machine $A$ and an arbitrary input $x$, does $A(x)$ halt?\nThis in effect describes the Halting problem, where $B$ is a universal Turing machine and $A$ can be any Turing Machine. It has been shown that there does not exist any such $B$ which solves this problem. Therefore the Halting problem cannot be decided, and is formally referred to as an undecidable problem.\nThe SAT Problem A Boolean formula $f$ accepts as input an $n$-bit string and outputs a $1$ (if it accepts the string), or a $0$ (if it rejects the string). Mathematically, $f:{0,1}^{n}\\xrightarrow{0,1}$.\nThe satisfiability(SAT) problem: Given a formula $f$ and an arbitrary $n$ bit string $x\\in{0,1}^{n}$, is $f(x)=1$?\nNote that the structure of SAT is subsumed by the structure of the Halting problem. If the decider to Halting problem ever halts, we can find out whether $f(x)=1$.\nEven though SAT can be quite hard to solve computationally (may have exponential runtime depending on the structure of the formula), we can construct an algorithm to solve it. Therefore, right off the bat, we observe that SAT is easier to solve (it is decidable) than the Halting problem.\nThe Reduction Let us finally have a look at how we would reduce SAT to the Halting problem.\nOur input $x$ is a SAT formula.\nWe construct a TM (Turing Machine) $T$ which accepts $x$ and does the following:\n$T$ iterates over all possible assignments to find a satisfying assignment. This may require exponential runtime in the size of the formula. If $T$ finds a satisfying assignment, it will halt. Otherwise, we put $T$ into an infinite loop. Our reduction $R(x)$ takes the SAT formula $x$ and returns an encoding of the above Turing machine $T$ with $x$. More precisely, we have something like $R(x)=\\left\u0026lt;\\lfloor T\\rfloor,x\\right\u0026gt;$. Note that $R(x)$ at this point can be compared to a compiled binary which has not yet been executed.\nWe pass $R(x)$ to $O_{\\text{halt}}$.\nIf $O_{\\text{halt}}$ returns yes, this implies $T$ halts on input $x$, which in turn implies $x$ has a satisfying assignment. Therefore $R(x)\\in\\text{HALT}\\implies x\\in\\text{SAT}$. If $O_{\\text{halt}}$ returns no, then $T$ does not halt on input $x$, which implies that $x$ does not have a satisfying assignment. Therefore $R(x)\\notin\\text{HALT}\\implies x\\notin\\text{SAT}$. We can take the contrapositive of this expression to obtain $x\\in\\text{SAT}\\implies R(x)\\in\\text{HALT}$. In step 2, we are simply constructing the TM $T$, not executing it. The reduction is the transformation of the SAT formula $x$ to an encoding of both the Turing Machine and the SAT formula, which is an instance of the Halting problem (HALT$_{TM}$ requires an arbitrary TM and an input string to the TM).\nTaxonomy of Polynomial-time reductions Here we only consider polynomial-time reductions, i.e.\nthe time taken to transform $x$ to $R(x)$, and the number of calls to $O_B$ are both polynomial. These are the most commonly studied types of reductions, and we look at three kinds of polynomial-time reductions. Karp Reductions / poly-time Many-one reductions These are the most restrictive type of polynomial reductions. Given a single input $x$, $R(x)$ produces a single instance $y$ such that $x\\in A\\iff y\\in B$. Therefore, we have to perform only one oracle call to $O_B$. The earlier reduction from SAT to HALT was a Karp reduction.\nWe can choose to strengthen the notion of Karp reductions to include weaker forms of reductions. For example, in logspace many-one reductions, we can compute $R(x)$ using just logarithmic space instead of polynomial time. Even more restrictive notions of reductions consider reductions computable by constant depth circuit classes.\nTruth Table Reductions These are reductions in which given a single input $x$, $R(x)$ produces a constant number of outputs $y_1,y_2,\\ldots, y_k$ to $B$. The output $O_A(x)$ can be expressed in terms of a function $f$ that combines the outputs $O_B(y_i)$ for $i\\in[1,\\ldots,k]$.\nLet us assume that $f$ outputs $1$ for the desired combination and $0$ otherwise. $$ x\\in A\\iff f\\left(y_1\\in B, y_1\\in B,\\ldots,y_k\\in B\\right)=1 $$ Here, $f$ is efficiently computable, and there are a constant ($k$) number of oracle calls to $O_B$.\nLet us consider an example. Consider two problems on a graph $G$ with a constant number of vertices $\\ell$.\n$A$: What is the minimum sized independent set for $G$? $B$: Does $G$ have an independent set of size $k$?\nThe reduction $A\\subseteq B$ would involve looping from $1$ to $\\ell$ and querying $B$ each time. The combination function would be an OR function. At the first yes instance of $B$, we return the value as the answer for $A$. If there is no independent set in $G$, the worst number of calls to $O_B$ is $\\ell$.\nCook Reductions / poly-time Turing Reductions Here, we are allowed a polynomial number of oracle calls and polynomial time for transforming the inputs. These are the most general form of reductions, and the other forms of reductions are restrictions of Cook reductions. In the example graph $G$ used for TT reductions, if we assume the number of vertices of $G$ to be a polynomial, then the reduction $A\\subseteq B$ using the same exact process would be a Cook reduction.\n$A\\subseteq_{m} B \\implies A\\subseteq_{t} B \\implies A\\subseteq_{T} B$ where, $\\subseteq_{m}$ denotes Karp reductions, $\\subseteq_{t}$ denotes Truth Table reductions, and $\\subseteq_{T}$ denotes Cook reductions.\nAside: From the nature of the examples, we can see that Karp reductions only extend to decision problems (problems with yes/no outputs). In contrast, Cook reductions can accommodate search/relation/optimization problems (problems with a set of outputs).\nBasic reductions in Computability Theory In this section, the reader is assumed to have familiarity with concepts of decidability and undecidability. Let us now proceed with some instances of reductions in computability theory.\nLet $A\\subseteq B$, and they are decision problems.\nIf $A$ is decidable, what can we say about $B$? If $A$ is semi-decidable, what can we say about $B$? If $A$ is undecidable, what can we say about $B$? We know that $B$ is at least as hard as $A$ since $A\\subseteq B$. Therefore in the first case, $B$ may be decidable, semi-decidable, or undecidable. In the second case, $B$ may be semi-decidable, or undecidable. In the third case, $B$ is undecidable.\nThe third case is of interest here - To show that a $B$ is undecidable, we must find a reduction from $A$ to $B$, where $A$ is already known to be undecidable. Note (Advanced): The reduction function must itself be a computable function.\nNow again, consider $A\\subseteq B$ where they are decision problems. We can conclude the following:\nIf $B$ is decidable, $A$ is decidable. If $B$ is semi-decidable, $A$ is semi-decidable. If $A$ is not decidable, then $B$ is not decidable. If $A$ is not semi-decidable, then $B$ is not semi-decidable. $A\\subseteq B$, then $\\bar{A}\\subseteq \\bar{B}$, where $A$ and $B$ are decidable problems. The first two statements are true, as discussed above, while 3 is the contrapositive of the first statement and the fourth statement is the contrapositive of the second statement. For the fifth statement, the proof is as follows:\n$x\\in \\bar{A} \\iff x\\notin A \\iff R(x)\\notin B \\iff R(x)\\in\\bar{B}$. Therefore, $x\\in \\bar{A} \\iff R(x)\\in\\bar{B}$.\nImmediately we see the power of formalizing the notion of reductions.\nNotions of reductions in Complexity Theory This section is for advanced readers only due to the number of prerequisites involved.\nA complexity class is a set of computational problems that can be solved using similar amounts of bounded resources (time, space, circuit depth, number of gates, etc.) on a given computational model (Turing machines, cellular automaton, etc.).\nThe complexity classes P, NP, and PSPACE are closed under Karp and logspace reductions. The complexity classes $L$ and $NL$ are closed only under logspace reductions. Closure has the following semantics - given a problem $A$ in a complexity class C, any problem $B$ such that $B\\subseteq A$ is also in C.\nWe now explore two interesting notions in complexity theory that arise from reductions.\nCompleteness For a bounded complexity class,\nComplete problems are the hardest problems inside their respective complexity class.\nA more formal definition of completeness is as follows:\nGiven a complexity class $C$ which is closed under reduction $r$, if there exists a problem $A$ in $C$ such that all other problems in $C$ are $r$-reducible to $A$, $A$ is said to be C-complete.\nFor example, an NP-complete problem is in NP, and all problems in NP are Karp-reducible to it. The notions of PSPACE-completeness and EXPTIME-completeness are similarly defined under Karp-reductions.\nThe role of reductions in this context can be understood through P-completeness. Consider any non-trivial decision problem in P (trivial problems are akin to constant functions). Every other non-trivial decision problem is Karp-reducible to it. Therefore every non-trivial decision problem is P-complete under Karp-reductions. This definition is essentially the same as P (minus the empty language and $\\Sigma^*$). Therefore, we come to the following conclusion:\nUnder Karp-reductions, the notion of P-completeness is semantically useless.\nTherefore, we use weaker forms of reductions such as logspace reductions or reductions using constant depth circuit computable functions to achieve a more meaningful notion of P-completeness.\nSelf reducibility Decision problems are yes/no problems where we ask if a solution exists. However, sometimes we also want to solve the corresponding search problem to find a solution (if one exists).\nIn the context of languages in NP, self-reducibility essentially states that if we can efficiently solve the decision version of a problem, we can also efficiently solve the search/optimization version of the problem. A more formal definition would be as follows - the search version of a problem Cook-reduces (polynomial-time Turing-reduces) to the decision version of the problem. Every NP-complete problem is self-reducible and downward self-reducible.\nConclusion This write-up aims to demystify a core technique used in theoretical computer science and provide a few contexts for its usage. For a more rigorous and formal introduction to this topic, please check out Michael Sipser\u0026rsquo;s excellent book \u0026ldquo;An Introduction to the Theory of Computation.\u0026rdquo; There are many more things to say about reductions, and this article barely scratches the surface on any of the topics it touches. However, I feel it would be prudent to stop at this point before it becomes any more of a rambling mess than it already is.\n","permalink":"https://theoreticles.netlify.app/posts/reductions/","summary":"This post assumes basic familiarity with Turing machines, P, NP, NP-completeness, decidability, and undecidability. Don\u0026rsquo;t hesitate to contact me via email if you need further clarification. Without further ado, let\u0026rsquo;s dive in.\nIntroduction Why do we need reductions? From Archimedes terrorizing the good folk of ancient Syracuse to Newton watching apples fall during a medieval plague, science has always progressed one \u0026lsquo;Eureka!\u0026rsquo; at a time. Romantic as these anecdotes may be, for mathematics, we can hardly look to Mother Nature for providing us the key insight to our proofs.","title":"An Introduction to Reductions"}]